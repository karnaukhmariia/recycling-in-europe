{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Recycling in Europe - Data Preprocessing Pipeline\n",
    "\n",
    "This notebook processes and cleans multiple datasets related to recycling rates and environmental factors across European countries. The datasets are combined to create a unified dataset for analysis.\n",
    "\n",
    "## Overview of Datasets:\n",
    "- **D1**: General recycling rates (overall waste recycling %)\n",
    "- **D2**: Recycling rates by waste type (glass, plastic, paper, metallic, wooden, packaging)\n",
    "- **D3**: Socioeconomic indicators (GDP per capita, urbanization, internet usage, renewable energy, etc.)\n",
    "- **CEI**: Circular economy indicators (private investments and output of circular economy sectors)\n",
    "- **ENV**: Environmental tax revenues (as % of GDP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING_RATE = 0.9\n",
    "EU_COUNTRIES = [\n",
    "    'Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', \n",
    "    'Czechia', 'Denmark', 'Estonia', 'Finland', 'France', \n",
    "    'Germany', 'Greece', 'Hungary', 'Ireland', 'Italy', \n",
    "    'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', \n",
    "    'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', \n",
    "    'Spain', 'Sweden'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(series: pd.Series) -> pd.Series:\n",
    "    # 1. interpolation for internal gaps\n",
    "    # 2. backward fill for leading NaNs\n",
    "    # 3. forward fill for trailing NaNs\n",
    "    return series.interpolate().bfill().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## D1 - general recylcing rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(\"raw_data/d1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1 = d1.copy()\n",
    "res_d1 = res_d1[['geo', 'TIME_PERIOD', \"OBS_VALUE\", \"OBS_FLAG\"]]\n",
    "res_d1 = res_d1.rename(columns={\"geo\": \"country_name\",\n",
    "                                \"TIME_PERIOD\": \"year\",\n",
    "                                \"OBS_VALUE\": \"recycling_rate\",\n",
    "                                \"OBS_FLAG\": \"flag\"})\n",
    "res_d1['country_name'] = res_d1['country_name'].astype(\"string\")\n",
    "res_d1['year'] = res_d1['year'].astype(int)\n",
    "res_d1['recycling_rate'] = res_d1['recycling_rate'].astype(float)\n",
    "res_d1['flag'] = res_d1['flag'].astype(\"string\")\n",
    "\n",
    "missing_rates_d1 = pd.DataFrame(res_d1.groupby(\"country_name\")[\"recycling_rate\"].apply(lambda x: x.isna().mean())).reset_index()\n",
    "missing_rates_d1.to_csv(\"processed_data/missing_value_rates_d1.csv\", index=False)\n",
    "\n",
    "values_to_drop_d1 = []\n",
    "for index, row in missing_rates_d1.iterrows():\n",
    "    if row['recycling_rate'] > MISSING_RATE:\n",
    "        values_to_drop_d1.append(row['country_name'])\n",
    "res_d1 = res_d1[~res_d1['country_name'].isin(values_to_drop_d1)]\n",
    "print(f\"Dropped countries in d1 (n={len(values_to_drop_d1)}):\", values_to_drop_d1)\n",
    "\n",
    "res_d1['recycling_rate_filled'] = res_d1.groupby(\"country_name\")[\"recycling_rate\"].transform(fill_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1.to_csv(\"processed_data/preprocessed_d1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## D2 - different types of waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = pd.read_csv(\"raw_data/d2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2 = d2.copy()\n",
    "res_d2 = res_d2[['geo', 'TIME_PERIOD', 'waste', 'OBS_VALUE', 'OBS_FLAG']]\n",
    "res_d2 = res_d2.rename(columns={\"geo\": \"country_name\",\n",
    "                                \"TIME_PERIOD\": \"year\",\n",
    "                                \"waste\": \"waste_type\",\n",
    "                                \"OBS_VALUE\": \"recycling_rate\",\n",
    "                                \"OBS_FLAG\": \"flag\"})\n",
    "res_d2['country_name'] = res_d2['country_name'].astype(\"string\")\n",
    "res_d2['year'] = res_d2['year'].astype(int)\n",
    "res_d2['waste_type'] = res_d2['waste_type'].astype(\"string\")\n",
    "res_d2['recycling_rate'] = res_d2['recycling_rate'].astype(float)\n",
    "res_d2['flag'] = res_d2['flag'].astype(\"string\")\n",
    "\n",
    "missing_rates_d2 = pd.DataFrame(res_d2.groupby([\"country_name\", 'waste_type'])[\"recycling_rate\"].apply(lambda x: x.isna().mean())).reset_index()\n",
    "missing_rates_d2.to_csv(\"processed_data/missing_value_rates_d2.csv\", index=False)\n",
    "\n",
    "values_to_drop_d2 = set()\n",
    "for index, row in missing_rates_d2.iterrows():\n",
    "    country_name = row['country_name']\n",
    "    waste_type = row['waste_type']\n",
    "    rate = row['recycling_rate']\n",
    "    if rate > MISSING_RATE:\n",
    "        values_to_drop_d2.add(country_name)\n",
    "        print(f\"{country_name}\\t{waste_type}\")\n",
    "\n",
    "res_d2 = res_d2[~res_d2['country_name'].isin(values_to_drop_d2)]\n",
    "print(f\"Dropped countries in d2 (n={len(values_to_drop_d2)}):\", values_to_drop_d2)\n",
    "\n",
    "res_d2['recycling_rate_filled'] = res_d2.groupby([\"country_name\", \"waste_type\"])[\"recycling_rate\"].transform(fill_missing)\n",
    "\n",
    "clean_names = {\n",
    "    \"Glass packaging\": \"glass\",\n",
    "    \"Metallic packaging\": \"metallic\",\n",
    "    \"Packaging\": \"packaging\",\n",
    "    \"Paper and cardboard packaging\": \"paper\",\n",
    "    \"Plastic packaging\": \"plastic\",\n",
    "    \"Wooden packaging\": \"wooden\"\n",
    "}\n",
    "res_d2['waste_type'] = res_d2['waste_type'].map(clean_names)\n",
    "\n",
    "res_d2 = res_d2.pivot(index=['country_name', 'year'], columns='waste_type', values=['recycling_rate_filled', \"recycling_rate\", \"flag\"]).reset_index()\n",
    "res_d2.columns = [f\"{val}_{waste}\" if waste else val for val, waste in res_d2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2.to_csv(\"processed_data/preprocessed_d2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2['year'].min(), res_d2['year'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Checking country overlap between D1 and D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in res_d1['country_name'].unique():\n",
    "    if c not in res_d2['country_name'].unique():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in res_d2['country_name'].unique():\n",
    "    if c not in res_d1['country_name'].unique():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_drop_d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_drop_d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = set(res_d1['country_name'].unique()).union(set(res_d2['country_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = pd.read_csv(\"raw_data/d3.csv\")\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in countries:\n",
    "    if c not in d3['Country Name'].unique():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = d3.replace(\"Slovak Republic\", \"Slovakia\")\n",
    "d3 = d3.replace(\"Kosovo\", \"Kosovo*\")\n",
    "d3 = d3.replace(\"Turkiye\", \"Türkiye\")\n",
    "for c in countries:\n",
    "    if c not in d3['Country Name'].unique():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = d3[d3['Country Name'].isin(countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d3 = d3.copy()\n",
    "res_d3 = res_d3.iloc[:-5, :]\n",
    "res_d3 = res_d3.rename(columns={\"Time\": \"year\",\n",
    "                                \"Country Name\": \"country_name\",\n",
    "                                \"GDP per capita (constant 2015 US$) [NY.GDP.PCAP.KD]\": \"gdp_per_capita\",\n",
    "                                \"Urban population (% of total population) [SP.URB.TOTL.IN.ZS]\": \"urban_population_pct\",\n",
    "                                \"Individuals using the Internet (% of population) [IT.NET.USER.ZS]\": \"internet_users_pct\",\n",
    "                                \"Renewable energy consumption (% of total final energy consumption) [EG.FEC.RNEW.ZS]\": \"renewable_energy_pct\",\n",
    "                                \"International tourism, number of arrivals [ST.INT.ARVL]\": \"tourism_arrivals\",\n",
    "                                \"Central government debt, total (% of GDP) [GC.DOD.TOTL.GD.ZS]\": \"government_debt_pct_gdp\",\n",
    "                                \"Population, total [SP.POP.TOTL]\": \"population_total\",\n",
    "                                \"Manufacturing, value added (% of GDP) [NV.IND.MANF.ZS]\": \"manufacturing_value_added_pct_gdp\",\n",
    "                                \"Government Effectiveness: Estimate [GE.EST]\": \"government_effectiveness_estimate\",\n",
    "                                \"Educational attainment, at least completed upper secondary, population 25+, total (%) (cumulative) [SE.SEC.CUAT.UP.ZS]\": \"highschool_completed_pct\",\n",
    "                                \"Households and NPISHs Final consumption expenditure per capita (constant 2015 US$) [NE.CON.PRVT.PC.KD]\": \"household_exp_percapita\"})\n",
    "\n",
    "# remove central government debt, too many missing values\n",
    "res_d3 = res_d3[[\"country_name\", \"year\", \"gdp_per_capita\", \"urban_population_pct\",\n",
    "                 \"internet_users_pct\", \"renewable_energy_pct\", \"tourism_arrivals\",\n",
    "                 \"population_total\", \"manufacturing_value_added_pct_gdp\",\n",
    "                 \"government_effectiveness_estimate\", \"highschool_completed_pct\", \"household_exp_percapita\"]]\n",
    "\n",
    "res_d3 = res_d3.replace(\"..\", np.nan)\n",
    "res_d3['country_name'] = res_d3['country_name'].astype(\"string\")\n",
    "res_d3['year'] = res_d3['year'].astype(int)\n",
    "res_d3['gdp_per_capita'] = res_d3['gdp_per_capita'].astype(float)\n",
    "res_d3['urban_population_pct'] = res_d3['urban_population_pct'].astype(float)\n",
    "res_d3['internet_users_pct'] = res_d3['internet_users_pct'].astype(float)\n",
    "res_d3['renewable_energy_pct'] = res_d3['renewable_energy_pct'].astype(float)\n",
    "res_d3['tourism_arrivals'] = res_d3['tourism_arrivals'].astype(float)\n",
    "res_d3['population_total'] = res_d3['population_total'].astype(float)\n",
    "res_d3['manufacturing_value_added_pct_gdp'] = res_d3['manufacturing_value_added_pct_gdp'].astype(float)\n",
    "res_d3['government_effectiveness_estimate'] = res_d3['government_effectiveness_estimate'].astype(float)\n",
    "\n",
    "values_to_drop_d3 = []\n",
    "missing_rates_d3 = pd.DataFrame()\n",
    "missing_rates_d3['country_name'] = res_d3['country_name'].unique()\n",
    "for col in res_d3.columns:\n",
    "    if col in [\"country_name\", \"year\"]:\n",
    "        continue\n",
    "    missing_rates_d3_col = pd.DataFrame(res_d3.groupby(\"country_name\")[col].apply(lambda x: x.isna().mean())).reset_index()\n",
    "    missing_rates_d3 = pd.merge(missing_rates_d3, missing_rates_d3_col, on='country_name', how='left', suffixes=('', f'_{col}'))\n",
    "    for index, row in missing_rates_d3_col.iterrows():\n",
    "        if row[col] > MISSING_RATE:\n",
    "            values_to_drop_d3.append(row['country_name'])\n",
    "            print(f\"{row['country_name']}\\t{col}\")\n",
    "missing_rates_d3.to_csv(\"processed_data/missing_value_rates_d3.csv\", index=False)\n",
    "res_d3 = res_d3[~res_d3['country_name'].isin(values_to_drop_d3)]\n",
    "\n",
    "\n",
    "for col in res_d3.columns:\n",
    "    if col in [\"country_name\", \"year\"]:\n",
    "        continue\n",
    "    res_d3[f\"{col}_filled\"] = res_d3.groupby(\"country_name\")[col].transform(fill_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d3.to_csv(\"processed_data/preprocessed_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## CEI - Private investment and gross value added (circular economy)\n",
    "\n",
    "**Dataset**: Circular Economy Indicators (CEI)\n",
    "\n",
    "**Description**: This dataset tracks the economic performance of circular economy sectors (recycling, repair, reuse) across European countries. It measures:\n",
    "- **Private investments**: Gross investments in tangible goods.\n",
    "- **Gross value added**: Economic output of circular economy sectors.\n",
    "\n",
    "**Unit**: Millions of euros. Will join with population to get Mill/pop.\n",
    "\n",
    "**Coverage**: All EU Member States, time series from 2005 onwards\n",
    "\n",
    "**Preprocessing steps**:\n",
    "1. Filter for investments as Millions of euros.\n",
    "2. Drop countries with >90% missing values\n",
    "3. Pivot wide format (indicators as columns)\n",
    "4. Save cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cei = pd.read_csv(\"raw_data/cei_cie012.csv\")\n",
    "cei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cei = cei.copy()\n",
    "res_cei = res_cei.rename(columns={\n",
    "    \"geo\": \"country_name\",\n",
    "    \"TIME_PERIOD\": \"year\",\n",
    "    \"OBS_VALUE\": \"value\",\n",
    "    \"indic_env\": \"indicator\"\n",
    "})\n",
    "res_cei = res_cei[res_cei[\"unit\"] == \"Million euro\"]\n",
    "res_cei = res_cei[res_cei[\"country_name\"].isin(EU_COUNTRIES)]\n",
    "\n",
    "res_cei = res_cei[[\"country_name\", \"year\", \"indicator\", \"value\"]]\n",
    "\n",
    "res_cei[\"country_name\"] = res_cei[\"country_name\"].astype(\"string\")\n",
    "res_cei[\"year\"] = res_cei[\"year\"].astype(int)\n",
    "res_cei[\"indicator\"] = res_cei[\"indicator\"].astype(\"string\")\n",
    "res_cei[\"value\"] = res_cei[\"value\"].astype(float)\n",
    "\n",
    "res_cei.loc[res_cei[\"indicator\"] == \"Gross value added\", \"indicator\"] = \"gross_val_add_mill\"\n",
    "res_cei.loc[res_cei[\"indicator\"] == \"Investment\", \"indicator\"] = \"priv_invest_mill\"\n",
    "\n",
    "print(\"Unique indicators:\", res_cei[\"indicator\"].unique())\n",
    "print(\"\\nData shape:\", res_cei.shape)\n",
    "res_cei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute missing rates and drop countries\n",
    "values_to_drop_cei = []\n",
    "missing_rates_cei = pd.DataFrame()\n",
    "\n",
    "for indicator in res_cei[\"indicator\"].unique():\n",
    "    subset = res_cei[res_cei[\"indicator\"] == indicator]\n",
    "    mr = pd.DataFrame(subset.groupby(\"country_name\")[\"value\"].apply(lambda x: x.isna().mean())).reset_index()\n",
    "    mr.columns = [\"country_name\", \"missing_rate\"]\n",
    "    mr[\"indicator\"] = indicator\n",
    "    \n",
    "    for _, row in mr.iterrows():\n",
    "        if row[\"missing_rate\"] > MISSING_RATE:\n",
    "            values_to_drop_cei.append(row[\"country_name\"])\n",
    "            print(f\"{row['country_name']}\\t{indicator}\")\n",
    "    \n",
    "    missing_rates_cei = pd.concat([missing_rates_cei, mr], ignore_index=True)\n",
    "\n",
    "missing_rates_cei.to_csv(\"processed_data/missing_value_rates_cei.csv\", index=False)\n",
    "values_to_drop_cei = list(set(values_to_drop_cei))\n",
    "print(f\"\\nDropped countries in CEI (n={len(values_to_drop_cei)}):\", values_to_drop_cei)\n",
    "\n",
    "res_cei = res_cei[~res_cei[\"country_name\"].isin(values_to_drop_cei)]\n",
    "\n",
    "print(f\"\\nRemaining countries: {res_cei['country_name'].nunique()}\")\n",
    "print(f\"Remaining rows: {len(res_cei)}\")\n",
    "res_cei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing per (country, indicator) time series\n",
    "res_cei['value_filled'] = res_cei.groupby(['country_name', 'indicator'])['value'].transform(fill_missing)\n",
    "\n",
    "# Pivot to wide format: each indicator becomes a column\n",
    "res_cei_wide = res_cei.pivot_table(\n",
    "    index=[\"country_name\", \"year\"],\n",
    "    columns=\"indicator\",\n",
    "    values=['value', 'value_filled'],\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "# Flatten multi-level columns\n",
    "res_cei_wide.columns = [f\"{ind}_{val}\" if ind else val \n",
    "                        for val, ind in res_cei_wide.columns]\n",
    "res_cei_wide.columns = res_cei_wide.columns.str.replace('_value', '')\n",
    "\n",
    "print(\"Final shape:\", res_cei_wide.shape)\n",
    "print(\"\\nColumns:\", list(res_cei_wide.columns))\n",
    "print(\"\\nMissing values:\")\n",
    "print(res_cei_wide.isna().sum())\n",
    "\n",
    "res_cei_wide.to_csv(\"processed_data/preprocessed_cei.csv\", index=False)\n",
    "print(\"\\nSaved to processed_data/preprocessed_cei.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## ENV - Environmental tax revenues\n",
    "\n",
    "**Dataset**: Environmental Tax Revenues (env_ac_tax)\n",
    "\n",
    "**Description**: This dataset contains government tax revenues from environmental taxes across European countries. Environmental taxes target specific activities with proven negative environmental impact. The dataset contains the taxes on **Pollution & Resources**. This taxes on emissions (air, water), waste management, water abstraction, and raw material extraction.\n",
    "\n",
    "**Unit**: Millions of euros. Will join with population to get Mill/pop.\n",
    "\n",
    "**Coverage**: All EU Member States, Iceland, Norway, Switzerland; time series from 1995 onwards\n",
    "\n",
    "**Preprocessing steps**:\n",
    "1. Select relevant columns (country, year, tax revenue)\n",
    "2. Drop countries with >90% missing values\n",
    "3. Apply interpolation and forward/backward fill to handle missing values\n",
    "4. Save cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pd.read_csv(\"raw_data/env_ac_tax.csv\")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV processing (long-format approach, then pivot)\n",
    "res_env = env.copy()\n",
    "\n",
    "# keep relevant columns\n",
    "res_env = res_env[['geo', 'tax', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "res_env = res_env.rename(columns={\n",
    "    'geo': 'country_name',\n",
    "    'TIME_PERIOD': 'year',\n",
    "    'OBS_VALUE': 'value'\n",
    "})\n",
    "\n",
    "# types\n",
    "res_env['country_name'] = res_env['country_name'].astype('string')\n",
    "res_env['tax'] = res_env['tax'].astype('string')\n",
    "res_env['year'] = res_env['year'].astype(int)\n",
    "res_env['value'] = res_env['value'].astype(float)\n",
    "\n",
    "res_env.loc[res_env[\"tax\"] == \"Total environmental taxes\", \"tax\"] = \"total_environm_tax_mill\"\n",
    "res_env.loc[res_env[\"tax\"] == \"Taxes on pollution/resources\", \"tax\"] = \"pollut_environm_tax_mill\"\n",
    "\n",
    "print(\"Unique indicators:\", res_env[\"tax\"].unique())\n",
    "print(\"\\nData shape:\", res_env.shape)\n",
    "res_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_drop_env = []\n",
    "missing_rates_env = pd.DataFrame()\n",
    "\n",
    "for tax in res_env[\"tax\"].unique():\n",
    "    subset = res_env[res_env[\"tax\"] == tax]\n",
    "    mr = pd.DataFrame(subset.groupby(\"country_name\")[\"value\"].apply(lambda x: x.isna().mean())).reset_index()\n",
    "    mr.columns = [\"country_name\", \"missing_rate\"]\n",
    "    mr[\"tax\"] = tax\n",
    "    \n",
    "    for _, row in mr.iterrows():\n",
    "        if row[\"missing_rate\"] > MISSING_RATE:\n",
    "            values_to_drop_env.append(row[\"country_name\"])\n",
    "            print(f\"To drop: {row['country_name']} due to {tax} ({row['missing_rate']:.2%})\")\n",
    "    \n",
    "    missing_rates_env = pd.concat([missing_rates_env, mr], ignore_index=True)\n",
    "\n",
    "missing_rates_env.to_csv(\"processed_data/missing_value_rates_env.csv\", index=False)\n",
    "values_to_drop_env = list(set(values_to_drop_env))\n",
    "res_env = res_env[~res_env[\"country_name\"].isin(values_to_drop_env)]\n",
    "\n",
    "print(f\"\\nDropped countries in ENV (n={len(values_to_drop_env)}):\", values_to_drop_env)\n",
    "print(f\"Remaining countries: {res_env['country_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_env['value_filled'] = res_env.groupby(['country_name', 'tax'])['value'].transform(fill_missing)\n",
    "\n",
    "res_env_wide = res_env.pivot_table(\n",
    "    index=[\"country_name\", \"year\"],\n",
    "    columns=\"tax\",\n",
    "    values=['value', 'value_filled'],\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "res_env_wide.columns = [f\"{ind}_{val}\" if ind else val \n",
    "                        for val, ind in res_env_wide.columns]\n",
    "\n",
    "res_env_wide.columns = res_env_wide.columns.str.replace('_value', '')\n",
    "\n",
    "print(\"\\nFinal shape:\", res_env_wide.shape)\n",
    "print(\"Columns:\", list(res_env_wide.columns))\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(res_env_wide.isna().sum())\n",
    "\n",
    "res_env_wide.to_csv(\"processed_data/preprocessed_env.csv\", index=False)\n",
    "print(\"\\nSaved to processed_data/preprocessed_env.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### D1 and D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1_d3 = pd.merge(res_d1, res_d3, on=['country_name', 'year'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1_d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1_d3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in res_d1.iterrows():\n",
    "    c = row['country_name']\n",
    "    y = row['year']\n",
    "    if res_d1_d3[(res_d1_d3['country_name'] == c) & (res_d1_d3['year'] == y)].empty:\n",
    "        print(c, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1_d3.to_csv(\"processed_data/preprocessed_d1_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### D2 and D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2_d3 = pd.merge(res_d2, res_d3, on=['country_name', 'year'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2_d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2_d3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in res_d2.iterrows():\n",
    "    c = row['country_name']\n",
    "    y = row['year']\n",
    "    if res_d2_d3[(res_d2_d3['country_name'] == c) & (res_d2_d3['year'] == y)].empty:\n",
    "        print(c, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2_d3.to_csv(\"processed_data/preprocessed_d2_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_unify_cols(df):\n",
    "    cols_to_drop = [c for c in df.columns if 'flag' in c]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    filled_cols = [c for c in df.columns if '_filled' in c]\n",
    "    \n",
    "    mapping = {}\n",
    "    for col in filled_cols:\n",
    "        clean_name = col.replace('_filled', '')\n",
    "        if clean_name in df.columns:\n",
    "            df = df.drop(columns=[clean_name])\n",
    "        mapping[col] = clean_name\n",
    "        \n",
    "    df = df.rename(columns=mapping)\n",
    "    return df\n",
    "\n",
    "dfs = [res_d1, res_d2, res_d3, res_cei_wide, res_env_wide]\n",
    "\n",
    "# Merge secuencial\n",
    "df_all = clean_and_unify_cols(dfs[0])\n",
    "for df in dfs[1:]:\n",
    "    df_all = df_all.merge(clean_and_unify_cols(df), on=['country_name', 'year'], how='outer')\n",
    "\n",
    "df_all = df_all[df_all['country_name'].isin(EU_COUNTRIES)].sort_values(['country_name', 'year'])\n",
    "\n",
    "\n",
    "df_all['total_environm_tax_per_capita'] = (df_all['total_environm_tax_mill'] * 1e6) / df_all['population_total']\n",
    "df_all = df_all.drop(columns=['total_environm_tax_mill'])\n",
    "\n",
    "df_all['pollut_environm_tax_per_capita'] = (df_all['pollut_environm_tax_mill'] * 1e6) / df_all['population_total']\n",
    "df_all = df_all.drop(columns=['pollut_environm_tax_mill'])\n",
    "\n",
    "df_all['gr_val_add_per_capita'] = (df_all['gross_val_add_mill'] * 1e6) / df_all['population_total']\n",
    "df_all = df_all.drop(columns=['gross_val_add_mill'])\n",
    "\n",
    "df_all['priv_inv_per_capita'] = (df_all['priv_invest_mill'] * 1e6) / df_all['population_total']\n",
    "df_all = df_all.drop(columns=['priv_invest_mill'])\n",
    "\n",
    "# Compute year range using non-null years\n",
    "years_non_null = df_all['year'].dropna().astype(int) if not df_all['year'].dropna().empty else pd.Series(dtype='int')\n",
    "min_year = int(years_non_null.min()) if not years_non_null.empty else None\n",
    "max_year = int(years_non_null.max()) if not years_non_null.empty else None\n",
    "\n",
    "print(f\"Final dataset shape: {df_all.shape}\")\n",
    "print(f\"Countries: {df_all['country_name'].nunique()}\")\n",
    "print(f\"Years: {min_year} - {max_year}\")\n",
    "print(f\"\\nMissing values:\\n{df_all.isna().sum().sum()} total missing values\")\n",
    "print(f\"\\nColumns: {len(df_all.columns)}\")\n",
    "df_all.to_csv(\"processed_data/preprocessed_all.csv\", index=False)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "# Result of preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "1. `res_d1`\n",
    "    1. Dataframe about the basic recycling rates of European countries\n",
    "    2. Filled values can be found in `recycling_rate_filled` (no missing values)\n",
    "    3. Dropped no country\n",
    "    4. Saved missing rates into csv (also the variable: `missing_rates_d1`), be aware, some countries had a lot of missing value\n",
    "        1. We can later change it to drop a country if it has missing values (or over a %).\n",
    "    5. Time: 2000-2023\n",
    "    6. Processed dataframe saved to csv.\n",
    "2. `res_d2`\n",
    "    1. Dataframe about the different recycling rates in European countries\n",
    "    2. Filled values can be found in new column (`recycling_rate_filled_{glass, metallic, packaging, paper, plastic, wooden}`)\n",
    "    3. Had to drop 7 countries: 100% missing value in one or more categories (most of them had 100% missing in all the categories)\n",
    "    4. Saved missing rates into csv (also the variable: `missing_rates_d2`), be aware (although better than D1)\n",
    "        1. We can later change it to drop a country if it has missing values (or over a %).\n",
    "    5. Time: 2012-2023\n",
    "    6. Processed dataframe saved to csv.\n",
    "3. `res_d3`\n",
    "    1. Dataframe about different indicators for European countries\n",
    "    2. Filled values can be found in a similar way.\n",
    "    3. Decided to drop government debt column, because more than 20 countries had it fully missing.\n",
    "    4. Dropped Bulgaria (100% missing in manufacturing), dropped Kosovo (100% missing in renewable, and tourism)\n",
    "    5. Saved missing rates into csv (also the variable: `missing_rates_d3`), be aware (although better than D1)\n",
    "        1. We can later change it to drop a country if it has missing values (or over a %).\n",
    "    6. Time: 2000-2024\n",
    "    7. Processed dataframe saved to csv.\n",
    "4. Joins\n",
    "    1. D1 and D3 -> `res_d1_d3`\n",
    "    2. D2 and D3 -> `res_d2_d3`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "First, we visualize the data of `res_d1` (recycling rate) before and after imputation to uncover any trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_country_dashboard(df, primary_metrics, secondary_metric=None, \n",
    "                           cols=4, title=\"Country Analysis\"):\n",
    "    \"\"\"\n",
    "    Creates a grid of subplots for countries with support for \n",
    "    multiple primary metrics and an optional secondary Y-axis.\n",
    "    \"\"\"\n",
    "    countries = df['country_name'].unique()\n",
    "    n_countries = len(countries)\n",
    "    rows = (n_countries // cols) + (1 if n_countries % cols > 0 else 0)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4), \n",
    "                             sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        ax1 = axes[i]\n",
    "        data = df[df['country_name'] == country].sort_values('year')\n",
    "        \n",
    "        # --- Plot Primary Metrics (Shared Left Axis) ---\n",
    "        for metric, style in primary_metrics.items():\n",
    "            sns.lineplot(data=data, x='year', y=metric, ax=ax1, \n",
    "                         color=style.get('color'), \n",
    "                         linestyle=style.get('ls', '-'), \n",
    "                         linewidth=style.get('lw', 2), \n",
    "                         label=style.get('label', metric), legend=False)\n",
    "        \n",
    "        ax1.set_title(country, fontweight='bold', fontsize=14)\n",
    "        ax1.set_ylim(-2, 100) # Assuming percentage/scaled data\n",
    "        \n",
    "        # --- Optional Secondary Metric (Independent Right Axis) ---\n",
    "        if secondary_metric:\n",
    "            ax2 = ax1.twinx()\n",
    "            sns.lineplot(data=data, x='year', y=secondary_metric, ax=ax2, \n",
    "                         color='purple', alpha=0.5, lw=1.5, legend=False)\n",
    "            \n",
    "            # Formatting right axis labels\n",
    "            if i % cols != cols - 1:\n",
    "                ax2.set_yticklabels([])\n",
    "            else:\n",
    "                ax2.set_ylabel(secondary_metric.replace('_', ' ').title())\n",
    "\n",
    "    # Clean up empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Universal Legend\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles1, labels1, loc='upper center', bbox_to_anchor=(0.5, 1.02), \n",
    "               ncol=min(len(labels1), 5), fontsize=12, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=20, y=1.05, fontweight='bold')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_v_imputed = {\n",
    "    'recycling_rate_filled': {'color': 'orange', 'lw': 2, 'alpha': 0.6, 'label': 'Imputed'},\n",
    "    'recycling_rate': {'color': 'teal', 'lw': 1.5, 'marker': 'o', 'ms': 4, 'label': 'Original'}\n",
    "}\n",
    "\n",
    "plot_country_dashboard(res_d1_d3, original_v_imputed, cols=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling government effectiveness to 0-100 for better visualization since it is between -2.5 and 2.5\n",
    "plot_df = res_d1_d3.copy()\n",
    "plot_df['gov_effectiveness_index'] = ((plot_df['government_effectiveness_estimate_filled'] + 2.5) / 5 * 100)\n",
    "\n",
    "# Primary metrics\n",
    "metrics_config = {\n",
    "    'recycling_rate_filled': {'color': 'orange', 'lw': 3, 'label': 'Recycling %'},\n",
    "    'urban_population_pct_filled': {'color': 'blue', 'ls': '--', 'label': 'Urban Pop %'},\n",
    "    'gov_effectiveness_index': {'color': 'black', 'ls': ':', 'label': 'Gov Index'},\n",
    "    'renewable_energy_pct_filled': {'color': 'green', 'ls': '-.', 'label': 'Renewable Energy %'},\n",
    "    'gdp_per_capita_filled': {'color': 'purple', 'ls': '-', 'label': 'GDP per Capita'}\n",
    "}\n",
    "\n",
    "plot_country_dashboard(plot_df, metrics_config, secondary_metric='gdp_per_capita_filled');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting 5 target countries for the management summary\n",
    "targets = ['Germany', 'Norway', 'Italy', 'Czechia', 'Spain']\n",
    "df_subset = res_d1_d3[res_d1_d3['country_name'].isin(targets)].copy()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", rc={\"axes.facecolor\": \"#f9f9f9\"})\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "palette = sns.color_palette(\"husl\", len(targets))\n",
    "\n",
    "line_plot = sns.lineplot(\n",
    "    data=df_subset, \n",
    "    x='year', \n",
    "    y='recycling_rate_filled', \n",
    "    hue='country_name', \n",
    "    linewidth=3.5,\n",
    "    marker='o',\n",
    "    markersize=8,\n",
    "    markeredgecolor='white',\n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "# EU 2030 target line\n",
    "plt.axhline(y=60, color='#d62728', linestyle='--', linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.text(2000.5, 61, 'EU 2030 Target (60%)', color='#d62728', \n",
    "         fontweight='bold', fontsize=12, va='bottom')\n",
    "\n",
    "plt.title(\"Path to 2030: Recycling Performance Tracking\", \n",
    "          fontsize=20, fontweight='bold', pad=25, loc='left')\n",
    "\n",
    "\n",
    "plt.ylabel(\"Recycling Rate (%)\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Reporting Year\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.legend(title=\"Target Countries\", title_fontsize='13', \n",
    "           fontsize='11', bbox_to_anchor=(1.02, 1), loc='upper left', frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis to identify drivers of recycling rates\n",
    "core_indicators = [\n",
    "    'recycling_rate_filled',\n",
    "    'gdp_per_capita_filled',\n",
    "    'urban_population_pct_filled',\n",
    "    'renewable_energy_pct_filled',\n",
    "    'government_effectiveness_estimate_filled']\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "df_corr = df_subset[core_indicators].corr()\n",
    "df_corr.columns = [c.replace('_filled', '').replace('_', ' ').title() for c in df_corr.columns]\n",
    "df_corr.index = [i.replace('_filled', '').replace('_', ' ').title() for i in df_corr.index]\n",
    "\n",
    "\n",
    "mask = np.triu(np.ones_like(df_corr, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    df_corr, \n",
    "    mask=mask, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap='RdYlGn', \n",
    "    center=0,\n",
    "    square=True, \n",
    "    linewidths=.5, \n",
    "    cbar_kws={\"shrink\": .7, \"label\": \"Correlation Coefficient ($r$)\"},\n",
    "    annot_kws={\"size\": 11, \"weight\": \"bold\"}\n",
    ")\n",
    "\n",
    "plt.title(\"What Drives Recycling?\\nRelationship Analysis for Selected Countries\", \n",
    "          fontsize=18, pad=25, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the same 5 target countries for horizontal comparison\n",
    "targets = ['Germany', 'Norway', 'Italy', 'Czechia', 'Spain']\n",
    "latest_year = res_d2_d3['year'].max()\n",
    "material_cols = [c for c in res_d2_d3.columns if 'recycling_rate_filled_' in c]\n",
    "material_labels = [m.replace('recycling_rate_filled_', '').title() for m in material_cols]\n",
    "\n",
    "# Plotting horizontal bar charts for each country\n",
    "fig, axes = plt.subplots(len(targets), 1, figsize=(12, len(targets) * 3), sharex=True)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "for i, country in enumerate(targets):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # extract data points\n",
    "    d2013 = res_d2_d3[(res_d2_d3['country_name'] == country) & (res_d2_d3['year'] == 2013)][material_cols].values.flatten()\n",
    "    dLatest = res_d2_d3[(res_d2_d3['country_name'] == country) & (res_d2_d3['year'] == latest_year)][material_cols].values.flatten()\n",
    "    \n",
    "    y_pos = np.arange(len(material_labels))\n",
    "\n",
    "    # Show the 2013 baseline\n",
    "    ax.barh(y_pos, d2013, color='#e0e0e0', label='2013 Rate', height=0.6)\n",
    "    \n",
    "    # Show the improvement (green) or decline (red)\n",
    "    diff = dLatest - d2013\n",
    "    colors = ['#2ca02c' if x >= 0 else '#d62728' for x in diff]\n",
    "    ax.barh(y_pos, diff, left=d2013, color=colors, label=f'Change to {latest_year}', height=0.6)\n",
    "\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(material_labels, fontweight='bold', fontsize=11)\n",
    "    ax.set_title(f\"Recycling Material Profile: {country}\", fontsize=15, fontweight='bold', loc='left', pad=10)\n",
    "    ax.set_xlim(-2, 100) # Added padding at 0 to see small bars\n",
    "    \n",
    "    # data labels\n",
    "    for j, val in enumerate(dLatest):\n",
    "        ax.text(val + 1, j, f\"{val:.1f}%\", va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    sns.despine(left=True, bottom=True)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.02), ncol=2, frameon=False, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## Dataset preperation for recycling rate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_drop = [\n",
    "    \"Bosnia and Herzegovina\",\n",
    "    \"Montenegro\",\n",
    "    \"North Macedonia\",\n",
    "    \"Türkiye\",\n",
    "    \"Serbia\",\n",
    "    \"Albania\"\n",
    "]\n",
    "\n",
    "res_d1_d3 = res_d1_d3[\n",
    "    ~res_d1_d3[\"country_name\"].isin(countries_to_drop)\n",
    "].copy()\n",
    "\n",
    "display(res_d1_d3)\n",
    "sorted(res_d1_d3[\"country_name\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_horizon_delta_dataset(\n",
    "    df,\n",
    "    horizon=7,\n",
    "    start_year=2000,\n",
    "    end_year=None,\n",
    "    target_col=\"recycling_rate_filled\",\n",
    "    feature_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a dataset where the target is the change in target_col over `horizon` years.\n",
    "    \"\"\"\n",
    "\n",
    "    if end_year is None:\n",
    "        end_year = df[\"year\"].max() - horizon\n",
    "\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\n",
    "            \"recycling_rate_filled\",\n",
    "            \"gdp_per_capita_filled\",\n",
    "            \"urban_population_pct_filled\",\n",
    "            \"internet_users_pct_filled\",\n",
    "            \"renewable_energy_pct_filled\",\n",
    "            \"tourism_arrivals_filled\",\n",
    "            \"population_total_filled\",\n",
    "            \"manufacturing_value_added_pct_gdp_filled\",\n",
    "            \"government_effectiveness_estimate_filled\"\n",
    "        ]\n",
    "\n",
    "    # anchor data (t)\n",
    "    df_t = df[\n",
    "        (df[\"year\"] >= start_year) &\n",
    "        (df[\"year\"] <= end_year)\n",
    "    ][[\"country_name\", \"year\"] + feature_cols].copy()\n",
    "\n",
    "    # future data (t + horizon)\n",
    "    df_t_future = df[\n",
    "        (df[\"year\"] >= start_year + horizon) &\n",
    "        (df[\"year\"] <= end_year + horizon)\n",
    "    ][[\"country_name\", \"year\", target_col]].copy()\n",
    "\n",
    "    df_t_future[\"year\"] -= horizon\n",
    "\n",
    "    # merge\n",
    "    merged = df_t.merge(\n",
    "        df_t_future,\n",
    "        on=[\"country_name\", \"year\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_future\")\n",
    "    )\n",
    "\n",
    "    # delta target\n",
    "    merged[f\"delta_{target_col}_{horizon}yr\"] = (\n",
    "        merged[f\"{target_col}_future\"] - merged[target_col]\n",
    "    )\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset with horizon recycling rates (in 7 years)\n",
    "df_7yr = build_horizon_delta_dataset(\n",
    "    df=res_d1_d3,\n",
    "    horizon=7,\n",
    ")\n",
    "\n",
    "display(df_7yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df_7yr[\"delta_recycling_rate_filled_7yr\"], bins=30)\n",
    "plt.xlabel(\"7-year change in recycling rate\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of 7-year recycling rate changes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "## Check for diminsihing returns \n",
    "\n",
    "We want to check that higher starting rate → smaller improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df_7yr[\"recycling_rate_filled\"],\n",
    "    df_7yr[\"delta_recycling_rate_filled_7yr\"],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.xlabel(\"Initial recycling rate\")\n",
    "plt.ylabel(\"7-year change\")\n",
    "plt.title(\"Diminishing returns check\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "## Split the data for delta = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation/test datasets\n",
    "train_df = df_7yr[df_7yr[\"year\"] <= 2012].copy()\n",
    "val_df   = df_7yr[df_7yr[\"year\"] > 2012].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining feature and target variables\n",
    "target = \"delta_recycling_rate_filled_7yr\"\n",
    "\n",
    "feature_cols = [\n",
    "    \"recycling_rate_filled\",\n",
    "    \"gdp_per_capita_filled\",\n",
    "    \"urban_population_pct_filled\",\n",
    "    \"internet_users_pct_filled\",\n",
    "    \"renewable_energy_pct_filled\",\n",
    "    \"tourism_arrivals_filled\",\n",
    "    \"population_total_filled\",\n",
    "    \"manufacturing_value_added_pct_gdp_filled\",\n",
    "    \"government_effectiveness_estimate_filled\"\n",
    "]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target]\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## Baseline Model: Linear regression (with regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Build a regression pipeline that scales features and fits a Ridge regression model\n",
    "baseline_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Generate predictions on validation set and evaluate model performance using MAE and R²\n",
    "y_pred = baseline_model.predict(X_val)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred))\n",
    "print(\"R²:\", r2_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coefficients of the regression model\n",
    "coefs = pd.Series(\n",
    "    baseline_model.named_steps[\"model\"].coef_,\n",
    "    index=X_train.columns\n",
    ").sort_values()\n",
    "\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted over actual 7-year change\n",
    "plt.scatter(y_val, y_pred, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-year change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "\n",
    "# Define helper functions\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, name=\"model\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    r2 = r2_score(y_val, preds)\n",
    "    return mae, r2, model, preds\n",
    "\n",
    "def cast_int_or_none(x):\n",
    "    return None if pd.isna(x) else int(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "## Non-linear Model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basline RF model \n",
    "rf_model = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "\n",
    "rf_mae, rf_r2, _, _ = evaluate_model(rf_model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"RF Baseline MAE:\", rf_mae)\n",
    "print(\"RF Baseline R²:\", rf_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual tuning to optimize hyperparameters (n_estimators, max_depth, min_samples_leaf, min_samples_split, max_features)\n",
    "results = []\n",
    "\n",
    "for n_estimators in [200, 400, 800]:\n",
    "    for max_depth in [None, 8, 12]:\n",
    "        for min_leaf in [1, 2, 4]:\n",
    "            for min_split in [2, 5, 10]:\n",
    "                for max_feat in [\"sqrt\", \"log2\", None]:\n",
    "\n",
    "                    rf = RandomForestRegressor(\n",
    "                        n_estimators=n_estimators,\n",
    "                        max_depth=max_depth,\n",
    "                        min_samples_leaf=min_leaf,\n",
    "                        min_samples_split=min_split,\n",
    "                        max_features=max_feat,\n",
    "                        random_state=42,\n",
    "                    )\n",
    "\n",
    "                    mae, r2, _, _ = evaluate_model(rf, X_train, y_train, X_val, y_val)\n",
    "\n",
    "                    results.append({\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"min_samples_leaf\": min_leaf,\n",
    "                        \"min_samples_split\": min_split,\n",
    "                        \"max_features\": max_feat,\n",
    "                        \"MAE\": mae,\n",
    "                        \"R2\": r2\n",
    "                    })\n",
    "\n",
    "rf_results = pd.DataFrame(results).sort_values(\"MAE\")\n",
    "\n",
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate best performing RF model (MAE, R2 & Importances)\n",
    "best_rf = RandomForestRegressor(\n",
    "    n_estimators=rf_results.iloc[0][\"n_estimators\"],\n",
    "    max_depth=cast_int_or_none(rf_results.iloc[0][\"max_depth\"]),\n",
    "    min_samples_leaf=rf_results.iloc[0][\"min_samples_leaf\"],\n",
    "    min_samples_split=rf_results.iloc[0][\"min_samples_split\"],\n",
    "    max_features=rf_results.iloc[0][\"max_features\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_rf_mae, best_rf_r2, best_rf, best_rf_preds = evaluate_model(best_rf, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Best RF MAE:\", best_rf_mae)\n",
    "print(\"Best RF R²:\", best_rf_r2)\n",
    "\n",
    "plt.scatter(y_val, best_rf_preds, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-year change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(best_rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "## Non-linear Model: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline Gradient Boosting model\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbr_mae, gbr_r2, _, _ = evaluate_model(gbr, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"GBR Baseline MAE:\", gbr_mae)\n",
    "print(\"GBR Baseline R²:\", gbr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual tuning to optimize hyperparameters (n_estimators, learning_rate, max_depth, min_samples_leaf, subsample)\n",
    "\n",
    "gbr_results = []\n",
    "\n",
    "for n_estimators in [200, 400, 800]:\n",
    "    for learning_rate in [0.03, 0.05, 0.1]:\n",
    "        for max_depth in [2, 3, 4]:\n",
    "            for min_leaf in [1, 3, 5]:\n",
    "                for subsample in [0.7, 0.9, 1.0]:\n",
    "\n",
    "                    gbr = GradientBoostingRegressor(\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth,\n",
    "                        min_samples_leaf=min_leaf,\n",
    "                        subsample=subsample,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    \n",
    "                    mae, r2, _, _ = evaluate_model(gbr, X_train, y_train, X_val, y_val)\n",
    "\n",
    "                    gbr_results.append({\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"min_samples_leaf\": min_leaf,\n",
    "                        \"subsample\": subsample,\n",
    "                        \"MAE\": mae,\n",
    "                        \"R2\": r2\n",
    "                    })\n",
    "\n",
    "gbr_results_df = (\n",
    "    pd.DataFrame(gbr_results)\n",
    "    .sort_values(\"MAE\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "gbr_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate best performing GBR model (MAE, R2 & Importances)\n",
    "\n",
    "best_gbr = GradientBoostingRegressor(\n",
    "    n_estimators=cast_int_or_none(gbr_results_df.iloc[0][\"n_estimators\"]),\n",
    "    learning_rate=gbr_results_df.iloc[0][\"learning_rate\"],\n",
    "    max_depth=cast_int_or_none(gbr_results_df.iloc[0][\"max_depth\"]),\n",
    "    min_samples_leaf=cast_int_or_none(gbr_results_df.iloc[0][\"min_samples_leaf\"]),\n",
    "    subsample=gbr_results_df.iloc[0][\"subsample\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_gbr_mae, best_gbr_r2, best_gbr, best_gbr_preds = evaluate_model(best_gbr, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Best GBR MAE:\", best_gbr_mae)\n",
    "print(\"Best GBR R²:\", best_gbr_r2)\n",
    "\n",
    "plt.scatter(y_val, best_gbr_preds, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-year change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(best_gbr.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "## Non-linear Model: Histogram-based Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basemodel HGB model\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    loss=\"absolute_error\",\n",
    "    max_iter=300,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hgb_mae, hgb_r2, _, _ = evaluate_model(hgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "print(\"HGB Baseline MAE:\", hgb_mae)\n",
    "print(\"HGB Baseline R²:\", hgb_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual tuning to optimize hyperparameters (max_iter, learning_rate, max_depth, min_samples_leaf)\n",
    "\n",
    "hgb_results = []\n",
    "\n",
    "for max_iter in [300, 600, 1000]:\n",
    "    for learning_rate in [0.03, 0.05, 0.1]:\n",
    "        for max_depth in [3, 5, 7]:\n",
    "            for min_samples_leaf in [10, 20, 40]:\n",
    "\n",
    "                hgb = HistGradientBoostingRegressor(\n",
    "                    max_iter=max_iter,\n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    random_state=42\n",
    "                )\n",
    "\n",
    "                mae, r2, _, _ = evaluate_model(hgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "                hgb_results.append({\n",
    "                    \"max_iter\": max_iter,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"max_depth\": max_depth,\n",
    "                    \"min_samples_leaf\": min_samples_leaf,\n",
    "                    \"MAE\": mae,\n",
    "                    \"R2\": r2\n",
    "                })\n",
    "\n",
    "hgb_results_df = (\n",
    "    pd.DataFrame(hgb_results)\n",
    "    .sort_values(\"MAE\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "hgb_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate best performing HGB model (MAE, R2)\n",
    "\n",
    "best_hgb = HistGradientBoostingRegressor(\n",
    "    max_iter=cast_int_or_none(hgb_results_df.iloc[0][\"max_iter\"]),\n",
    "    learning_rate=hgb_results_df.iloc[0][\"learning_rate\"],\n",
    "    max_depth=cast_int_or_none(hgb_results_df.iloc[0][\"max_depth\"]),\n",
    "    min_samples_leaf=cast_int_or_none(hgb_results_df.iloc[0][\"min_samples_leaf\"]),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_hgb_mae, best_hgb_r2, best_hgb, best_hgb_preds = evaluate_model(best_hgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Best HGB MAE:\", best_hgb_mae)\n",
    "print(\"Best hgb R²:\", best_hgb_r2)\n",
    "\n",
    "plt.scatter(y_val, best_hgb_preds, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-year change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "## Non-linear Model: Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define Baseline XGB model\n",
    "xgb = XGBRegressor(\n",
    "    objective=\"reg:absoluteerror\", \n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_mae, xgb_r2, _, _ = evaluate_model(xgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"XGB Baseline MAE:\", xgb_mae)\n",
    "print(\"XGB Baseline R²:\", xgb_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Manual tuning to optimize hyperparameters (n_estimators, learning_rate, max_depth, subsample, colsample_bytree)\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "for n_estimators in [300, 600, 1000]:\n",
    "    for learning_rate in [0.03, 0.05, 0.1]:\n",
    "        for max_depth in [3, 4, 5]:\n",
    "            for subsample in [0.7, 0.9, 1.0]:\n",
    "                for colsample in [0.7, 0.9, 1.0]:\n",
    "\n",
    "                    xgb = XGBRegressor(\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth,\n",
    "                        subsample=subsample,\n",
    "                        colsample_bytree=colsample,\n",
    "                        random_state=42,\n",
    "                    )\n",
    "\n",
    "                    mae, r2, _, _ = evaluate_model(xgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "                    xgb_results.append({\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"subsample\": subsample,\n",
    "                        \"colsample_bytree\": colsample,\n",
    "                        \"MAE\": mae,\n",
    "                        \"R2\": r2\n",
    "                    })\n",
    "\n",
    "xgb_results_df = (\n",
    "    pd.DataFrame(xgb_results)\n",
    "    .sort_values(\"MAE\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "xgb_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate best performing XGB model (MAE, R2 & Importances)\n",
    "\n",
    "best_xgb = XGBRegressor(\n",
    "    n_estimators=cast_int_or_none(xgb_results_df.iloc[0][\"n_estimators\"]),\n",
    "    learning_rate=xgb_results_df.iloc[0][\"learning_rate\"],\n",
    "    max_depth=cast_int_or_none(xgb_results_df.iloc[0][\"max_depth\"]),\n",
    "    subsample=xgb_results_df.iloc[0][\"subsample\"],\n",
    "    colsample_bytree=xgb_results_df.iloc[0][\"colsample_bytree\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_xgb_mae, best_xgb_r2, best_xgb, best_xgb_preds = evaluate_model(best_xgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Best XGB MAE:\", best_xgb_mae)\n",
    "print(\"Best XGB R²:\", best_xgb_r2)\n",
    "\n",
    "plt.scatter(y_val, best_xgb_preds, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-yeaxr change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(best_xgb.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "## Would we see better MAE with 5 years delta (instead of 7)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resplitting dataset for delta of 5 years\n",
    "\n",
    "df_5yr = build_horizon_delta_dataset(\n",
    "    df=res_d1_d3,\n",
    "    horizon=5\n",
    ")\n",
    "\n",
    "train_df_5 = df_5yr[df_5yr[\"year\"] <= 2014]\n",
    "val_df_5   = df_5yr[df_5yr[\"year\"] > 2014]\n",
    "\n",
    "target_5 = \"delta_recycling_rate_filled_5yr\"\n",
    "\n",
    "X_train_5 = train_df_5[feature_cols]\n",
    "y_train_5 = train_df_5[target_5]\n",
    "\n",
    "X_val_5 = val_df_5[feature_cols]\n",
    "y_val_5 = val_df_5[target_5]\n",
    "\n",
    "# Evaluatign previous optimized models for delta of 5 years\n",
    "best_rf_5_mae , best_rf_5_r2, _, _ = evaluate_model(best_rf,  X_train_5, y_train_5, X_val_5, y_val_5, \"RF (5yr)\")\n",
    "best_gbr_5_mae , best_gbr_5_r2, _, _ =evaluate_model(best_gbr, X_train_5, y_train_5, X_val_5, y_val_5, \"GBR (5yr)\")\n",
    "best_hgb_5_mae , best_hgb_5_r2, _, _ =evaluate_model(best_hgb, X_train_5, y_train_5, X_val_5, y_val_5, \"HGB (5yr)\")\n",
    "best_xgb_5_mae , best_xgb_5_r2, _, _ =evaluate_model(best_xgb, X_train_5, y_train_5, X_val_5, y_val_5, \"XGB (5yr)\")\n",
    "\n",
    "results_5yr = {\n",
    "    \"RF (5yr)\": (best_rf_5_mae, best_rf_5_r2),\n",
    "    \"GBR (5yr)\": (best_gbr_5_mae, best_gbr_5_r2),\n",
    "    \"HGB (5yr)\": (best_hgb_5_mae, best_hgb_5_r2),\n",
    "    \"XGB (5yr)\": (best_xgb_5_mae, best_xgb_5_r2),\n",
    "}\n",
    "\n",
    "for model, (mae, r2) in results_5yr.items():\n",
    "    print(f\"{model:10s} | MAE: {mae:.3f} | R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "Answer is no but we might need to retune the models for 5 years delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "## Predict recycling rates for dataset in 2030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions__df = res_d1_d3[res_d1_d3[\"year\"] == res_d1_d3[\"year\"].max()].copy()\n",
    "\n",
    "# Features for prediction\n",
    "X_latest = predictions__df[feature_cols]\n",
    "\n",
    "# Predict 7-year delta from latest year\n",
    "predicted_delta = best_xgb.predict(X_latest)\n",
    "\n",
    "# Compute projected 2030 rate\n",
    "predictions__df[\"predicted_delta_2030\"] = predicted_delta\n",
    "predictions__df[\"predicted_rate_2030\"] = predictions__df[\"recycling_rate_filled\"] + predicted_delta\n",
    "\n",
    "eu_target = 60 \n",
    "predictions__df[\"meets_target\"] = predictions__df[\"predicted_rate_2030\"] >= eu_target\n",
    "\n",
    "projection_2030 = predictions__df[\n",
    "    [\"country_name\", \"recycling_rate_filled\", \"predicted_delta_2030\", \"predicted_rate_2030\", \"meets_target\"]\n",
    "].sort_values(\"predicted_rate_2030\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "projection_2030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show predicted deltas by 2030\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(projection_2030[\"predicted_delta_2030\"], bins=7, kde=True, color='skyblue')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted 7-year increase in recycling rate\")\n",
    "plt.ylabel(\"Number of countries\")\n",
    "plt.title(\"Distribution of Predicted Recycling Rate Increase by 2030\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predicted final recycling rates by 2030\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(projection_2030[\"predicted_rate_2030\"], bins=9, kde=True, color='green')\n",
    "plt.axvline(eu_target, color='red', linestyle='--', label=f'EU Target ({eu_target}%)')\n",
    "plt.xlabel(\"Predicted recycling rate in 2030 (%)\")\n",
    "plt.ylabel(\"Number of countries\")\n",
    "plt.title(\"Distribution of Predicted Recycling Rates in 2030\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Current vs preidcted recycling rates by 2030\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(\n",
    "    x=\"recycling_rate_filled\",\n",
    "    y=\"predicted_rate_2030\",\n",
    "    hue=\"meets_target\",\n",
    "    data=projection_2030,\n",
    "    palette={True: 'green', False: 'red'},\n",
    "    s=100\n",
    ")\n",
    "plt.plot([0, 100], [0, 100], 'k--', alpha=0.7)  # diagonal\n",
    "plt.xlabel(\"Current recycling rate (%)\")\n",
    "plt.ylabel(\"Predicted rate in 2030 (%)\")\n",
    "plt.title(\"Current vs Predicted Recycling Rates\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "## Countries with biggest predicted improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show countries with biggets predicted improvements \n",
    "top_increases = projection_2030.sort_values(\"predicted_delta_2030\", ascending=False).head(10)\n",
    "top_increases[[\"country_name\", \"recycling_rate_filled\", \"predicted_delta_2030\", \"predicted_rate_2030\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "## Countries by outcome analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most contriubting variables to recycling rates deltas\n",
    "importances = best_xgb.feature_importances_\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feat_imp_df, palette=\"viridis\")\n",
    "plt.title(\"Global Feature Importance (XGBoost)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(best_xgb, X_latest)\n",
    "shap_values = explainer(X_latest)\n",
    "\n",
    "# Global summary\n",
    "shap.summary_plot(shap_values, X_latest, feature_names=feature_cols)\n",
    "\n",
    "# Mean absolute SHAP value per feature\n",
    "shap_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"mean_abs_shap\": np.abs(shap_values.values).mean(axis=0)\n",
    "}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "print(shap_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Determine grid size\n",
    "n_features = len(feature_cols)\n",
    "n_cols = 3 \n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
    "axes = axes.flatten()  # flatten for easy indexing\n",
    "\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    sns.kdeplot(achievers[feature], label=\"Achievers\", fill=True, ax=axes[i])\n",
    "    sns.kdeplot(non_achievers[feature], label=\"Non-Achievers\", fill=True, ax=axes[i])\n",
    "    axes[i].set_title(f\"{feature} distribution\")\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Remove empty subplots if any\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
