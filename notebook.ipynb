{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Recycling in Europe\n",
    "\n",
    "This notebook processes and cleans multiple datasets related to recycling rates and environmental factors across European countries. The datasets are combined to create a unified dataset for analysis.\n",
    "\n",
    "## Overview of Datasets:\n",
    "- **D1**: General recycling rates (overall waste recycling %)\n",
    "- **D2**: Recycling rates by waste type (glass, plastic, paper, metallic, wooden, packaging)\n",
    "- **D3**: Socioeconomic indicators (GDP per capita, urbanization, internet usage, renewable energy, etc.)\n",
    "- **CEI**: Circular economy indicators (private investments and output of circular economy sectors)\n",
    "- **ENV**: Environmental tax revenues (as % of GDP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib.lines import Line2D\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING_RATE = 0.9\n",
    "EU_COUNTRIES = [\n",
    "    'Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', \n",
    "    'Czechia', 'Denmark', 'Estonia', 'Finland', 'France', \n",
    "    'Germany', 'Greece', 'Hungary', 'Ireland', 'Italy', \n",
    "    'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', \n",
    "    'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', \n",
    "    'Spain', 'Sweden'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7bd23",
   "metadata": {},
   "source": [
    "Generally, we used the following function to fill in missing values. The general logic is to use linear interpolation for intermediate gaps, followed by backard and forward filling to handle the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(series: pd.Series) -> pd.Series:\n",
    "    # 1. interpolation for internal gaps\n",
    "    # 2. backward fill for leading NaNs\n",
    "    # 3. forward fill for trailing NaNs\n",
    "    return series.interpolate().bfill().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## D1 - general recylcing rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0a0b1",
   "metadata": {},
   "source": [
    "- **Why this dataset?**\n",
    "    - We chose this dataset to serve as the primary benchmark for the analysis, providing high-level overview of how effectively different European nations manage their total waste output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(\"raw_data/d1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy\n",
    "res_d1 = d1.copy()\n",
    "\n",
    "# selecting the columns and renaming\n",
    "res_d1 = res_d1[['geo', 'TIME_PERIOD', \"OBS_VALUE\", \"OBS_FLAG\"]]\n",
    "res_d1 = res_d1.rename(columns={\"geo\": \"country_name\",\n",
    "                                \"TIME_PERIOD\": \"year\",\n",
    "                                \"OBS_VALUE\": \"recycling_rate\",\n",
    "                                \"OBS_FLAG\": \"flag\"})\n",
    "\n",
    "# types\n",
    "res_d1['country_name'] = res_d1['country_name'].astype(\"string\")\n",
    "res_d1['year'] = res_d1['year'].astype(int)\n",
    "res_d1['recycling_rate'] = res_d1['recycling_rate'].astype(float)\n",
    "res_d1['flag'] = res_d1['flag'].astype(\"string\")\n",
    "\n",
    "# missing rates for later\n",
    "missing_rates_d1 = pd.DataFrame(res_d1.groupby(\"country_name\")[\"recycling_rate\"].apply(lambda x: x.isna().mean())).reset_index()\n",
    "missing_rates_d1.to_csv(\"processed_data/missing_value_rates_d1.csv\", index=False)\n",
    "\n",
    "# filtering based on missing rates\n",
    "values_to_drop_d1 = []\n",
    "for index, row in missing_rates_d1.iterrows():\n",
    "    if row['recycling_rate'] > MISSING_RATE:\n",
    "        values_to_drop_d1.append(row['country_name'])\n",
    "res_d1 = res_d1[~res_d1['country_name'].isin(values_to_drop_d1)]\n",
    "print(f\"Dropped countries in d1 (n={len(values_to_drop_d1)}):\", values_to_drop_d1)\n",
    "\n",
    "# filling missing values\n",
    "res_d1['recycling_rate_filled'] = res_d1.groupby(\"country_name\")[\"recycling_rate\"].transform(fill_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1.to_csv(\"processed_data/preprocessed_d1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## D2 - different types of waste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a195d98",
   "metadata": {},
   "source": [
    "- Why this dataset?\n",
    "    - This dataset was selected to provide necessary granularity, allowing us to identify if countries excel in processing specific materials. It helps reveal structural differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = pd.read_csv(\"raw_data/d2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy\n",
    "res_d2 = d2.copy()\n",
    "\n",
    "# selecting the columns and renaming\n",
    "res_d2 = res_d2[['geo', 'TIME_PERIOD', 'waste', 'OBS_VALUE', 'OBS_FLAG']]\n",
    "res_d2 = res_d2.rename(columns={\"geo\": \"country_name\",\n",
    "                                \"TIME_PERIOD\": \"year\",\n",
    "                                \"waste\": \"waste_type\",\n",
    "                                \"OBS_VALUE\": \"recycling_rate\",\n",
    "                                \"OBS_FLAG\": \"flag\"})\n",
    "\n",
    "# types\n",
    "res_d2['country_name'] = res_d2['country_name'].astype(\"string\")\n",
    "res_d2['year'] = res_d2['year'].astype(int)\n",
    "res_d2['waste_type'] = res_d2['waste_type'].astype(\"string\")\n",
    "res_d2['recycling_rate'] = res_d2['recycling_rate'].astype(float)\n",
    "res_d2['flag'] = res_d2['flag'].astype(\"string\")\n",
    "\n",
    "# missing rates for later\n",
    "missing_rates_d2 = pd.DataFrame(res_d2.groupby([\"country_name\", 'waste_type'])[\"recycling_rate\"].apply(lambda x: x.isna().mean())).reset_index()\n",
    "missing_rates_d2.to_csv(\"processed_data/missing_value_rates_d2.csv\", index=False)\n",
    "\n",
    "# filtering based on missing rates\n",
    "values_to_drop_d2 = set()\n",
    "for index, row in missing_rates_d2.iterrows():\n",
    "    country_name = row['country_name']\n",
    "    waste_type = row['waste_type']\n",
    "    rate = row['recycling_rate']\n",
    "    if rate > MISSING_RATE:\n",
    "        values_to_drop_d2.add(country_name)\n",
    "        print(f\"{country_name}\\t{waste_type}\")\n",
    "res_d2 = res_d2[~res_d2['country_name'].isin(values_to_drop_d2)]\n",
    "print(f\"Dropped countries in d2 (n={len(values_to_drop_d2)}):\", values_to_drop_d2)\n",
    "\n",
    "# filling missing values\n",
    "res_d2['recycling_rate_filled'] = res_d2.groupby([\"country_name\", \"waste_type\"])[\"recycling_rate\"].transform(fill_missing)\n",
    "\n",
    "# pivoting waste types\n",
    "clean_names = {\n",
    "    \"Glass packaging\": \"glass\",\n",
    "    \"Metallic packaging\": \"metallic\",\n",
    "    \"Packaging\": \"packaging\",\n",
    "    \"Paper and cardboard packaging\": \"paper\",\n",
    "    \"Plastic packaging\": \"plastic\",\n",
    "    \"Wooden packaging\": \"wooden\"\n",
    "}\n",
    "res_d2['waste_type'] = res_d2['waste_type'].map(clean_names)\n",
    "res_d2 = res_d2.pivot(index=['country_name', 'year'], columns='waste_type', values=['recycling_rate_filled', \"recycling_rate\", \"flag\"]).reset_index()\n",
    "res_d2.columns = [f\"{val}_{waste}\" if waste else val for val, waste in res_d2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2.to_csv(\"processed_data/preprocessed_d2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Creating overlapping countries set for D1 and D2 (both Eurostat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bfd845",
   "metadata": {},
   "source": [
    "We do this because D3 contains significantly more countries, and we would like to filter for these countries first there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in res_d1['country_name'].unique():\n",
    "    if c not in res_d2['country_name'].unique():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in res_d2['country_name'].unique():\n",
    "    if c not in res_d1['country_name'].unique():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_drop_d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_drop_d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = set(res_d1['country_name'].unique()).union(set(res_d2['country_name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## D3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f5a4e",
   "metadata": {},
   "source": [
    "- Why this dataset?\n",
    "    - We included these World Development Indicators to investigate potential drivers of recycling performance, specifically whether economic wealth (GDP) and infrastructure density (urbanization) correlate with higher recylcing rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = pd.read_csv(\"raw_data/d3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75415a",
   "metadata": {},
   "source": [
    "Which countries are not present in D3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in countries:\n",
    "    if c not in d3['Country Name'].unique():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually searched for the differences\n",
    "d3 = d3.replace(\"Slovak Republic\", \"Slovakia\")\n",
    "d3 = d3.replace(\"Kosovo\", \"Kosovo*\")\n",
    "d3 = d3.replace(\"Turkiye\", \"TÃ¼rkiye\")\n",
    "for c in countries:\n",
    "    if c not in d3['Country Name'].unique():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for the countries in d1 and d2\n",
    "d3 = d3[d3['Country Name'].isin(countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d3 = d3.copy()\n",
    "res_d3 = res_d3.iloc[:-5, :] # last 5 rows are aggregates/metadata\n",
    "\n",
    "# renaming columns\n",
    "res_d3 = res_d3.rename(columns={\"Time\": \"year\",\n",
    "                                \"Country Name\": \"country_name\",\n",
    "                                \"GDP per capita (constant 2015 US$) [NY.GDP.PCAP.KD]\": \"gdp_per_capita\",\n",
    "                                \"Urban population (% of total population) [SP.URB.TOTL.IN.ZS]\": \"urban_population_pct\",\n",
    "                                \"Individuals using the Internet (% of population) [IT.NET.USER.ZS]\": \"internet_users_pct\",\n",
    "                                \"Renewable energy consumption (% of total final energy consumption) [EG.FEC.RNEW.ZS]\": \"renewable_energy_pct\",\n",
    "                                \"International tourism, number of arrivals [ST.INT.ARVL]\": \"tourism_arrivals\",\n",
    "                                \"Central government debt, total (% of GDP) [GC.DOD.TOTL.GD.ZS]\": \"government_debt_pct_gdp\",\n",
    "                                \"Population, total [SP.POP.TOTL]\": \"population_total\",\n",
    "                                \"Manufacturing, value added (% of GDP) [NV.IND.MANF.ZS]\": \"manufacturing_value_added_pct_gdp\",\n",
    "                                \"Government Effectiveness: Estimate [GE.EST]\": \"government_effectiveness_estimate\",\n",
    "                                \"Educational attainment, at least completed upper secondary, population 25+, total (%) (cumulative) [SE.SEC.CUAT.UP.ZS]\": \"highschool_completed_pct\",\n",
    "                                \"Households and NPISHs Final consumption expenditure per capita (constant 2015 US$) [NE.CON.PRVT.PC.KD]\": \"household_exp_percapita\"})\n",
    "\n",
    "# remove central government debt, too many missing values\n",
    "res_d3 = res_d3[[\"country_name\", \"year\", \"gdp_per_capita\", \"urban_population_pct\",\n",
    "                 \"internet_users_pct\", \"renewable_energy_pct\", \"tourism_arrivals\",\n",
    "                 \"population_total\", \"manufacturing_value_added_pct_gdp\",\n",
    "                 \"government_effectiveness_estimate\", \"highschool_completed_pct\", \"household_exp_percapita\"]]\n",
    "\n",
    "# types\n",
    "res_d3 = res_d3.replace(\"..\", np.nan)\n",
    "res_d3['country_name'] = res_d3['country_name'].astype(\"string\")\n",
    "res_d3['year'] = res_d3['year'].astype(int)\n",
    "res_d3['gdp_per_capita'] = res_d3['gdp_per_capita'].astype(float)\n",
    "res_d3['urban_population_pct'] = res_d3['urban_population_pct'].astype(float)\n",
    "res_d3['internet_users_pct'] = res_d3['internet_users_pct'].astype(float)\n",
    "res_d3['renewable_energy_pct'] = res_d3['renewable_energy_pct'].astype(float)\n",
    "res_d3['tourism_arrivals'] = res_d3['tourism_arrivals'].astype(float)\n",
    "res_d3['population_total'] = res_d3['population_total'].astype(float)\n",
    "res_d3['manufacturing_value_added_pct_gdp'] = res_d3['manufacturing_value_added_pct_gdp'].astype(float)\n",
    "res_d3['government_effectiveness_estimate'] = res_d3['government_effectiveness_estimate'].astype(float)\n",
    "res_d3['household_exp_percapita'] = res_d3['household_exp_percapita'].astype(float)\n",
    "res_d3['highschool_completed_pct'] = res_d3['highschool_completed_pct'].astype(float)\n",
    "\n",
    "# filtering based on missing rates\n",
    "values_to_drop_d3 = []\n",
    "missing_rates_d3 = pd.DataFrame()\n",
    "missing_rates_d3['country_name'] = res_d3['country_name'].unique()\n",
    "for col in res_d3.columns:\n",
    "    if col in [\"country_name\", \"year\"]:\n",
    "        continue\n",
    "    missing_rates_d3_col = pd.DataFrame(res_d3.groupby(\"country_name\")[col].apply(lambda x: x.isna().mean())).reset_index()\n",
    "    missing_rates_d3 = pd.merge(missing_rates_d3, missing_rates_d3_col, on='country_name', how='left', suffixes=('', f'_{col}'))\n",
    "    for index, row in missing_rates_d3_col.iterrows():\n",
    "        if row[col] > MISSING_RATE:\n",
    "            values_to_drop_d3.append(row['country_name'])\n",
    "            print(f\"{row['country_name']}\\t{col}\")\n",
    "missing_rates_d3.to_csv(\"processed_data/missing_value_rates_d3.csv\", index=False)\n",
    "res_d3 = res_d3[~res_d3['country_name'].isin(values_to_drop_d3)]\n",
    "\n",
    "# filling missing values\n",
    "for col in res_d3.columns:\n",
    "    if col in [\"country_name\", \"year\"]:\n",
    "        continue\n",
    "    res_d3[f\"{col}_filled\"] = res_d3.groupby(\"country_name\")[col].transform(fill_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d3.to_csv(\"processed_data/preprocessed_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## CEI - Private investment and gross value added (circular economy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7624e0b",
   "metadata": {},
   "source": [
    "- Why this dataset?\n",
    "    - This dataset was chosen to measure the economic commitment to sustainability, tracking the actual private investment and financial output of the circular economy sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cei = pd.read_csv(\"raw_data/cei_cie012.csv\")\n",
    "cei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cei = cei.copy()\n",
    "\n",
    "# renaming countries\n",
    "res_cei = res_cei.rename(columns={\n",
    "    \"geo\": \"country_name\",\n",
    "    \"TIME_PERIOD\": \"year\",\n",
    "    \"OBS_VALUE\": \"value\",\n",
    "    \"indic_env\": \"indicator\"\n",
    "})\n",
    "\n",
    "# filtering for relevant indicators, units, and countries\n",
    "res_cei = res_cei[res_cei[\"unit\"] == \"Million euro\"]\n",
    "res_cei = res_cei[res_cei[\"country_name\"].isin(EU_COUNTRIES)]\n",
    "res_cei = res_cei[[\"country_name\", \"year\", \"indicator\", \"value\"]]\n",
    "\n",
    "# types\n",
    "res_cei[\"country_name\"] = res_cei[\"country_name\"].astype(\"string\")\n",
    "res_cei[\"year\"] = res_cei[\"year\"].astype(int)\n",
    "res_cei[\"indicator\"] = res_cei[\"indicator\"].astype(\"string\")\n",
    "res_cei[\"value\"] = res_cei[\"value\"].astype(float)\n",
    "\n",
    "# renaming indicators\n",
    "res_cei.loc[res_cei[\"indicator\"] == \"Gross value added\", \"indicator\"] = \"gross_val_add_mill\"\n",
    "res_cei.loc[res_cei[\"indicator\"] == \"Investment\", \"indicator\"] = \"priv_invest_mill\"\n",
    "\n",
    "print(\"Unique indicators:\", res_cei[\"indicator\"].unique())\n",
    "print(\"\\nData shape:\", res_cei.shape)\n",
    "res_cei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute missing rates and drop countries\n",
    "values_to_drop_cei = []\n",
    "missing_rates_cei = pd.DataFrame()\n",
    "\n",
    "for indicator in res_cei[\"indicator\"].unique():\n",
    "    subset = res_cei[res_cei[\"indicator\"] == indicator]\n",
    "    mr = pd.DataFrame(subset.groupby(\"country_name\")[\"value\"].apply(lambda x: x.isna().mean())).reset_index()\n",
    "    mr.columns = [\"country_name\", \"missing_rate\"]\n",
    "    mr[\"indicator\"] = indicator\n",
    "    \n",
    "    for _, row in mr.iterrows():\n",
    "        if row[\"missing_rate\"] > MISSING_RATE:\n",
    "            values_to_drop_cei.append(row[\"country_name\"])\n",
    "            print(f\"{row['country_name']}\\t{indicator}\")\n",
    "    \n",
    "    missing_rates_cei = pd.concat([missing_rates_cei, mr], ignore_index=True)\n",
    "\n",
    "missing_rates_cei.to_csv(\"processed_data/missing_value_rates_cei.csv\", index=False)\n",
    "values_to_drop_cei = list(set(values_to_drop_cei))\n",
    "print(f\"\\nDropped countries in CEI (n={len(values_to_drop_cei)}):\", values_to_drop_cei)\n",
    "\n",
    "res_cei = res_cei[~res_cei[\"country_name\"].isin(values_to_drop_cei)]\n",
    "\n",
    "print(f\"\\nRemaining countries: {res_cei['country_name'].nunique()}\")\n",
    "print(f\"Remaining rows: {len(res_cei)}\")\n",
    "res_cei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing per (country, indicator) time series\n",
    "res_cei['value_filled'] = res_cei.groupby(['country_name', 'indicator'])['value'].transform(fill_missing)\n",
    "\n",
    "# Pivot to wide format: each indicator becomes a column\n",
    "res_cei_wide = res_cei.pivot_table(\n",
    "    index=[\"country_name\", \"year\"],\n",
    "    columns=\"indicator\",\n",
    "    values=['value', 'value_filled'],\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "# Flatten multi-level columns\n",
    "res_cei_wide.columns = [f\"{ind}_{val}\" if ind else val \n",
    "                        for val, ind in res_cei_wide.columns]\n",
    "res_cei_wide.columns = res_cei_wide.columns.str.replace('_value', '')\n",
    "\n",
    "print(\"Final shape:\", res_cei_wide.shape)\n",
    "print(\"\\nColumns:\", list(res_cei_wide.columns))\n",
    "print(\"\\nMissing values:\")\n",
    "print(res_cei_wide.isna().sum())\n",
    "\n",
    "res_cei_wide.to_csv(\"processed_data/preprocessed_cei.csv\", index=False)\n",
    "print(\"\\nSaved to processed_data/preprocessed_cei.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## ENV - Environmental tax revenues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7491d8f",
   "metadata": {},
   "source": [
    "- Why this dataset?\n",
    "    - We selected this dataset to analyse the role of fiscal policy, acting as a proxy for government intervention. It allows us to test whether higher environmental taxes successfully help nations to adopt better recycling practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pd.read_csv(\"raw_data/env_ac_tax.csv\")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV processing (long-format approach, then pivot)\n",
    "res_env = env.copy()\n",
    "\n",
    "# keep relevant columns\n",
    "res_env = res_env[['geo', 'tax', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "res_env = res_env.rename(columns={\n",
    "    'geo': 'country_name',\n",
    "    'TIME_PERIOD': 'year',\n",
    "    'OBS_VALUE': 'value'\n",
    "})\n",
    "\n",
    "# types\n",
    "res_env['country_name'] = res_env['country_name'].astype('string')\n",
    "res_env['tax'] = res_env['tax'].astype('string')\n",
    "res_env['year'] = res_env['year'].astype(int)\n",
    "res_env['value'] = res_env['value'].astype(float)\n",
    "\n",
    "res_env.loc[res_env[\"tax\"] == \"Total environmental taxes\", \"tax\"] = \"total_environm_tax_mill\"\n",
    "res_env.loc[res_env[\"tax\"] == \"Taxes on pollution/resources\", \"tax\"] = \"pollut_environm_tax_mill\"\n",
    "\n",
    "print(\"Unique indicators:\", res_env[\"tax\"].unique())\n",
    "print(\"\\nData shape:\", res_env.shape)\n",
    "res_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_drop_env = []\n",
    "missing_rates_env = pd.DataFrame()\n",
    "\n",
    "for tax in res_env[\"tax\"].unique():\n",
    "    subset = res_env[res_env[\"tax\"] == tax]\n",
    "    mr = pd.DataFrame(subset.groupby(\"country_name\")[\"value\"].apply(lambda x: x.isna().mean())).reset_index()\n",
    "    mr.columns = [\"country_name\", \"missing_rate\"]\n",
    "    mr[\"tax\"] = tax\n",
    "    \n",
    "    for _, row in mr.iterrows():\n",
    "        if row[\"missing_rate\"] > MISSING_RATE:\n",
    "            values_to_drop_env.append(row[\"country_name\"])\n",
    "            print(f\"To drop: {row['country_name']} due to {tax} ({row['missing_rate']:.2%})\")\n",
    "    \n",
    "    missing_rates_env = pd.concat([missing_rates_env, mr], ignore_index=True)\n",
    "\n",
    "missing_rates_env.to_csv(\"processed_data/missing_value_rates_env.csv\", index=False)\n",
    "values_to_drop_env = list(set(values_to_drop_env))\n",
    "res_env = res_env[~res_env[\"country_name\"].isin(values_to_drop_env)]\n",
    "\n",
    "print(f\"\\nDropped countries in ENV (n={len(values_to_drop_env)}):\", values_to_drop_env)\n",
    "print(f\"Remaining countries: {res_env['country_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_env['value_filled'] = res_env.groupby(['country_name', 'tax'])['value'].transform(fill_missing)\n",
    "\n",
    "res_env_wide = res_env.pivot_table(\n",
    "    index=[\"country_name\", \"year\"],\n",
    "    columns=\"tax\",\n",
    "    values=['value', 'value_filled'],\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "res_env_wide.columns = [f\"{ind}_{val}\" if ind else val \n",
    "                        for val, ind in res_env_wide.columns]\n",
    "\n",
    "res_env_wide.columns = res_env_wide.columns.str.replace('_value', '')\n",
    "\n",
    "print(\"\\nFinal shape:\", res_env_wide.shape)\n",
    "print(\"Columns:\", list(res_env_wide.columns))\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(res_env_wide.isna().sum())\n",
    "\n",
    "res_env_wide.to_csv(\"processed_data/preprocessed_env.csv\", index=False)\n",
    "print(\"\\nSaved to processed_data/preprocessed_env.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### D1 and D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1_d3 = pd.merge(res_d1, res_d3, on=['country_name', 'year'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1_d3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in res_d1.iterrows():\n",
    "    c = row['country_name']\n",
    "    y = row['year']\n",
    "    if res_d1_d3[(res_d1_d3['country_name'] == c) & (res_d1_d3['year'] == y)].empty:\n",
    "        print(c, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ed5a4",
   "metadata": {},
   "source": [
    "These values were dropped before, we can move on and save the dataset. Countries from the intersection (see before) are all present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d1_d3.to_csv(\"processed_data/preprocessed_d1_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### D2 and D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2_d3 = pd.merge(res_d2, res_d3, on=['country_name', 'year'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2_d3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in res_d2.iterrows():\n",
    "    c = row['country_name']\n",
    "    y = row['year']\n",
    "    if res_d2_d3[(res_d2_d3['country_name'] == c) & (res_d2_d3['year'] == y)].empty:\n",
    "        print(c, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fb2b0",
   "metadata": {},
   "source": [
    "There were dropped before, we can move on and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d2_d3.to_csv(\"processed_data/preprocessed_d2_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372174ce",
   "metadata": {},
   "source": [
    "## All datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_unify_cols(df):\n",
    "    cols_to_drop = [c for c in df.columns if 'flag' in c]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    filled_cols = [c for c in df.columns if '_filled' in c]\n",
    "    \n",
    "    mapping = {}\n",
    "    for col in filled_cols:\n",
    "        clean_name = col.replace('_filled', '')\n",
    "        if clean_name in df.columns:\n",
    "            df = df.drop(columns=[clean_name])\n",
    "        mapping[col] = clean_name\n",
    "        \n",
    "    df = df.rename(columns=mapping)\n",
    "    return df\n",
    "\n",
    "dfs = [res_d1, res_d2, res_d3, res_cei_wide, res_env_wide]\n",
    "\n",
    "# Merge secuencial\n",
    "df_all = clean_and_unify_cols(dfs[0])\n",
    "for df in dfs[1:]:\n",
    "    df_all = df_all.merge(clean_and_unify_cols(df), on=['country_name', 'year'], how='outer')\n",
    "\n",
    "df_all = df_all[df_all['country_name'].isin(EU_COUNTRIES)].sort_values(['country_name', 'year'])\n",
    "\n",
    "df_all['total_environm_tax_per_capita'] = (df_all['total_environm_tax_mill'] * 1e6) / df_all['population_total']\n",
    "df_all = df_all.drop(columns=['total_environm_tax_mill'])\n",
    "\n",
    "df_all['pollut_environm_tax_per_capita'] = (df_all['pollut_environm_tax_mill'] * 1e6) / df_all['population_total']\n",
    "df_all = df_all.drop(columns=['pollut_environm_tax_mill'])\n",
    "\n",
    "df_all['gr_val_add_per_capita'] = (df_all['gross_val_add_mill'] * 1e6) / df_all['population_total']\n",
    "df_all = df_all.drop(columns=['gross_val_add_mill'])\n",
    "\n",
    "df_all['priv_inv_per_capita'] = (df_all['priv_invest_mill'] * 1e6) / df_all['population_total']\n",
    "df_all = df_all.drop(columns=['priv_invest_mill'])\n",
    "\n",
    "# Compute year range using non-null years\n",
    "years_non_null = df_all['year'].dropna().astype(int) if not df_all['year'].dropna().empty else pd.Series(dtype='int')\n",
    "min_year = int(years_non_null.min()) if not years_non_null.empty else None\n",
    "max_year = int(years_non_null.max()) if not years_non_null.empty else None\n",
    "\n",
    "print(f\"Final dataset shape: {df_all.shape}\")\n",
    "print(f\"Countries: {df_all['country_name'].nunique()}\")\n",
    "print(f\"Years: {min_year} - {max_year}\")\n",
    "print(f\"\\nMissing values:\\n{df_all.isna().sum().sum()} total missing values\")\n",
    "print(f\"\\nColumns: {len(df_all.columns)}\")\n",
    "df_all.to_csv(\"processed_data/preprocessed_all.csv\", index=False)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## Result of preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "1. `res_d1`\n",
    "    1. Dataframe about the basic recycling rates of European countries\n",
    "    2. Time: 2000-2023\n",
    "    3. Preprocessing steps:\n",
    "        1. Load the raw data.\n",
    "        2. Select the important columns and rename them.\n",
    "        3. Set the correct types for the columns.\n",
    "        4. Calculated and saved missing rates\n",
    "            1. It can be found in `missing_rates_d1`.\n",
    "        5. Dropped values based on missing rates.\n",
    "            1. No countries were dropped.\n",
    "        6. Filled missing values based on logic mentioned in the Functions section.\n",
    "            1. Filled values can be found in the `recycling_rate_filled` column.\n",
    "        7. Saved the preprocessed data.\n",
    "\n",
    "2. `res_d2`\n",
    "    1. Dataframe about the different recycling rates in European countries\n",
    "    2. Time: 2012-2023\n",
    "    3. Preprocessing steps:\n",
    "        1. Load the raw data.\n",
    "        2. Select the important columns and rename them.\n",
    "        3. Set the correct types for the columns.\n",
    "        4. Calculated and saved missing rates\n",
    "            1. It can be found in `missing_rates_d2`.\n",
    "        5. Dropped values based on missing rates.\n",
    "            1. Had to drop several countries.\n",
    "        6. Filled missing values based on logic mentioned in the Functions section.\n",
    "            1. After pivotong (see step 7.): Filled values can be found in new column (`recycling_rate_filled_{glass, metallic, packaging, paper, plastic, wooden}`)\n",
    "        7. Pivoted the data.\n",
    "            1. Before, there were two columns: the waste type and the recyling rate for the corresponding waste type.\n",
    "            2. Now, we have a column for each waste type.\n",
    "        7. Saved the preprocessed data.\n",
    "3. `res_d3`\n",
    "    1. Dataframe about different indicators for European countries\n",
    "    2. Filled values can be found in a similar way.\n",
    "    3. Decided to drop government debt column, because more than 20 countries had it fully missing.\n",
    "    4. Dropped Bulgaria (100% missing in manufacturing), dropped Kosovo (100% missing in renewable, and tourism)\n",
    "    5. Saved missing rates into csv (also the variable: `missing_rates_d3`), be aware (although better than D1)\n",
    "        1. We can later change it to drop a country if it has missing values (or over a %).\n",
    "    6. Time: 2000-2024\n",
    "    3. Preprocessing steps:\n",
    "        1. Load the raw data.\n",
    "        2. Filter for countries that were both present in D1 and D2.\n",
    "            1. We do this because D3 contains significantly more countries, and it's easier to start the preprocessing with less countries.\n",
    "            2. Some countries were not found in D3.\n",
    "                1. Manually checked the raw files, these countries used different names, we had to change these.\n",
    "        3. Drop last 5 rows, because it is aggregate/metadata.\n",
    "        4. Rename the columns.\n",
    "        5. Set the correct types for the columns.\n",
    "        6. Calculated and saved missing rates.\n",
    "            1. It can be found in `missing_rates_d3`.\n",
    "        7. Dropped values based on missing rates.\n",
    "            1. Dropped Bulgaria (100% missing in manufacturing), dropped Kosovo (100% missing in renewable, and tourism)\n",
    "            2. Decided to drop the government debt column, because more than 20 countries had it fully missing.\n",
    "        8. Filled missing values based on logic mentioned in the Functions section.\n",
    "        9. Saved the preprocessed data.\n",
    "4. Circular Economy Indicators (CEI)\n",
    "    1. Description* This dataset tracks the economic performance of circular economy sectors (recycling, repair, reuse) across European countries. It measures:\n",
    "        1. Private investments: Gross investments in tangible goods.\n",
    "        2. Gross value added: Economic output of circular economy sectors.\n",
    "    2. Unit: Millions of euros. Will join with population to get Mill/pop.\n",
    "    4. Coverage: All EU Member States, time series from 2005 onwards\n",
    "    5. Preprocessing steps:\n",
    "        1. Filter for investments as Millions of euros.\n",
    "        2. Drop countries with missing values\n",
    "        3. Pivot wide format (indicators as columns)\n",
    "        4. Save cleaned dataset\n",
    "    6. Results: Found no missing values in the dataframe. The resulting dataframe contains valid datapoints for all EU Member States.\n",
    "5. Environmental Tax Revenues\n",
    "    1. Description: This dataset contains government tax revenues from environmental taxes across European countries. Environmental taxes target specific activities with proven negative environmental impact. The dataset contains also the specific taxes on Pollution & Resources. This taxes on emissions (air, water), waste management, water abstraction, and raw material extraction.\n",
    "    2. Unit: Millions of euros. Will join with population to get Mill/pop.\n",
    "    3. Coverage: All EU Member States, Iceland, Norway, Switzerland; time series from 1995 onwards\n",
    "    4. Preprocessing steps:\n",
    "        1. Select relevant columns (country, year, tax revenue)\n",
    "        2. Drop countries with missing values\n",
    "        3. Pivot wide format (tax as columns)\n",
    "        4. Save cleaned dataset\n",
    "    5. Result: Found missing values for both taxes, the environmental and the specific in pollution and resources, on Liechtenstein and Switzerland. Dropped values of Liechtenstein as there were 100% missing values for `pollut_environm_tax_mill`.\n",
    "6. Joins\n",
    "    1. D1 and D3 -> `res_d1_d3`\n",
    "    2. D2 and D3 -> `res_d2_d3`\n",
    "    3. All five -> `df_all`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "The following part contains the visualisations relevant to the preprocessing. The first two graphs investigate the data in detail, they are not meant to be presented as they are quite crowded and contain a lot of elements. They are necessary however, to obtain understanding of the data on the country level by each variable.\n",
    "\n",
    "First, we visualize the data of `res_d1_d3` (recycling rate) before and after imputation to uncover any trends in the data. As expected, we see that the missing data is on the edges, and is extrapolated with straight lines as a result of the fill. For the purpose of the analysis, we chose only EU countries as they are the most relevant. These countries do not have a high proportion of missing values.\n",
    "\n",
    "The second graph shows each variable in more detail. We look at the already filled data and do not notice any concerning patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_country_dashboard(df, primary_metrics, secondary_metric=None, \n",
    "                           cols=4, title=\"Country Analysis\"):\n",
    "    \"\"\"\n",
    "    Creates a grid of subplots for countries with support for \n",
    "    multiple primary metrics and an optional secondary Y-axis.\n",
    "    \"\"\"\n",
    "    countries = df['country_name'].unique()\n",
    "    n_countries = len(countries)\n",
    "    rows = (n_countries // cols) + (1 if n_countries % cols > 0 else 0)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4), \n",
    "                             sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        ax1 = axes[i]\n",
    "        data = df[df['country_name'] == country].sort_values('year')\n",
    "        \n",
    "        # Primary metrics, shared left axis\n",
    "        for metric, style in primary_metrics.items():\n",
    "            sns.lineplot(data=data, x='year', y=metric, ax=ax1, \n",
    "                         color=style.get('color'), \n",
    "                         linestyle=style.get('ls', '-'), \n",
    "                         linewidth=style.get('lw', 2), \n",
    "                         label=style.get('label', metric), legend=False)\n",
    "        \n",
    "        ax1.set_title(country, fontweight='bold', fontsize=14)\n",
    "        ax1.set_ylim(-2, 100) # Assuming percentage/scaled data\n",
    "        \n",
    "        # Optional secondary metrics, right axis\n",
    "        if secondary_metric:\n",
    "            ax2 = ax1.twinx()\n",
    "            sns.lineplot(data=data, x='year', y=secondary_metric, ax=ax2, \n",
    "                         color='purple', alpha=0.5, lw=1.5, legend=False)\n",
    "            \n",
    "            # Formatting right axis labels\n",
    "            if i % cols != cols - 1:\n",
    "                ax2.set_yticklabels([])\n",
    "            else:\n",
    "                ax2.set_ylabel(secondary_metric.replace('_', ' ').title())\n",
    "\n",
    "    # Clean up empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Universal Legend\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles1, labels1, loc='upper center', bbox_to_anchor=(0.5, 1.02), \n",
    "               ncol=min(len(labels1), 5), fontsize=12, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=20, y=1.05, fontweight='bold')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original vs imputed recycling rates\n",
    "original_v_imputed = {\n",
    "    'recycling_rate_filled': {'color': 'orange', 'lw': 2, 'alpha': 0.6, 'label': 'Imputed'},\n",
    "    'recycling_rate': {'color': 'teal', 'lw': 1.5, 'marker': 'o', 'ms': 4, 'label': 'Original'}\n",
    "}\n",
    "\n",
    "plot_country_dashboard(res_d1_d3, original_v_imputed, cols=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling government effectiveness to 0-100 for better visualization since it is between -2.5 and 2.5\n",
    "plot_df = res_d1_d3.copy()\n",
    "plot_df['gov_effectiveness_index'] = ((plot_df['government_effectiveness_estimate_filled'] + 2.5) / 5 * 100)\n",
    "\n",
    "# Primary metrics\n",
    "metrics_config = {\n",
    "    'recycling_rate_filled': {'color': 'orange', 'lw': 3, 'label': 'Recycling %'},\n",
    "    'urban_population_pct_filled': {'color': 'blue', 'ls': '--', 'label': 'Urban Pop %'},\n",
    "    'government_effectiveness_estimate_filled': {'color': 'black', 'ls': ':', 'label': 'Gov Index'},\n",
    "    'renewable_energy_pct_filled': {'color': 'green', 'ls': '-.', 'label': 'Renewable Energy %'},\n",
    "    'gdp_per_capita_filled': {'color': 'purple', 'ls': '-', 'label': 'GDP per Capita'},\n",
    "    'manufacturing_value_added_pct_gdp_filled': {'color': 'brown', 'ls': '-', 'label': 'Manufacturing % GDP'},\n",
    "    'population_total_filled': {'color': 'gray', 'ls': '-', 'label': 'Total Population'},\n",
    "    'tourism_arrivals_filled': {'color': 'cyan', 'ls': '-', 'label': 'Tourism Arrivals'},\n",
    "    'internet_users_pct_filled': {'color': 'magenta', 'ls': '--', 'label': 'Internet Users %'},\n",
    "    'household_exp_percapita_filled': {'color': 'red', 'ls': '-', 'label': 'Household Exp per Capita'}\n",
    "}\n",
    "\n",
    "plot_country_dashboard(plot_df, metrics_config, secondary_metric='gdp_per_capita_filled');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1efc9",
   "metadata": {},
   "source": [
    "The following graphs are the first ones to be included in the management summary. Because the data is so high-dimensional: many countries, years and variables, including all of it in a visualization makes it difficult to follow. Hence, we chose 5 countries in order to represent the case in different parts of the EU. We also plot the 2030 EU Recycling target to see how these countries compare against it.\n",
    "\n",
    "All countries, except Sweden, show a steady increase in the recycling rates with Germany having met the target for several years now. In fact, Sweden even shows a decline in the recycling rates starting from 2020, possibly due to Covid-19. All countries show a plateau/slight decline in the years following Covid-19. This is likely due to the fact that the pandemic required a large number of resources, that had to be diverted elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting 5 target countries for the management summary\n",
    "targets = ['Germany', 'Sweden', 'Italy', 'Czechia', 'Greece']\n",
    "df_subset = res_d1_d3[res_d1_d3['country_name'].isin(targets)].copy()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", rc={\"axes.facecolor\": \"#f9f9f9\"})\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "palette = sns.color_palette(\"husl\", len(targets))\n",
    "\n",
    "line_plot = sns.lineplot(\n",
    "    data=df_subset, \n",
    "    x='year', \n",
    "    y='recycling_rate_filled', \n",
    "    hue='country_name', \n",
    "    linewidth=3.5,\n",
    "    marker='o',\n",
    "    markersize=8,\n",
    "    markeredgecolor='white',\n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "# EU 2030 target line\n",
    "plt.axhline(y=60, color='#d62728', linestyle='--', linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.text(2000.5, 61, 'EU 2030 Target (60%)', color='#d62728', \n",
    "         fontweight='bold', fontsize=12, va='bottom')\n",
    "\n",
    "plt.title(\"Path to 2030: Recycling Performance Tracking\", \n",
    "          fontsize=20, fontweight='bold', pad=25, loc='left')\n",
    "\n",
    "\n",
    "plt.ylabel(\"Recycling Rate (%)\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Reporting Year\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.legend(title=\"Target Countries\", title_fontsize='13', \n",
    "           fontsize='11', bbox_to_anchor=(1.02, 1), loc='upper left', frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../recycling-in-europe/figures/recycling_performance_2030.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b45b87",
   "metadata": {},
   "source": [
    "Then, we move on to correlation analysis between the variables. This is important for several reasons, first it gives a preliminary overview which variables might be more important in our analysis. Second, it gives an overview of multicollinearity in the data that is important for modelling and regularization.\n",
    "\n",
    "The graph shows that variables such as government effectiveness and percentage of internet users are strong drivers of recycling rates. Surprisingly, the percentage of renewable energy being utilized does not show a high correlation coefficient. Of course, it is important to not that correlation does not imply causation, and that this is just the preliminary step to obtain an overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis to identify drivers of recycling rates\n",
    "core_indicators = [\n",
    "    'recycling_rate_filled',\n",
    "    'gdp_per_capita_filled',\n",
    "    'urban_population_pct_filled',\n",
    "    'renewable_energy_pct_filled',\n",
    "    'government_effectiveness_estimate_filled',\n",
    "    'manufacturing_value_added_pct_gdp_filled',\n",
    "    'population_total_filled',\n",
    "    'tourism_arrivals_filled',\n",
    "    'internet_users_pct_filled',\n",
    "    'household_exp_percapita_filled']\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "df_corr = res_d1_d3[core_indicators].corr()\n",
    "df_corr.columns = [c.replace('_filled', '').replace('_', ' ').title() for c in df_corr.columns]\n",
    "df_corr.index = [i.replace('_filled', '').replace('_', ' ').title() for i in df_corr.index]\n",
    "\n",
    "\n",
    "mask = np.triu(np.ones_like(df_corr, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    df_corr, \n",
    "    mask=mask, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap='RdYlGn', \n",
    "    center=0,\n",
    "    square=True, \n",
    "    linewidths=.5, \n",
    "    cbar_kws={\"shrink\": .7, \"label\": \"Correlation Coefficient ($r$)\"},\n",
    "    annot_kws={\"size\": 11, \"weight\": \"bold\"}\n",
    ")\n",
    "\n",
    "plt.title(\"What Drives Recycling?\\nRelationship Analysis for Selected Countries\", \n",
    "          fontsize=18, pad=25, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../recycling-in-europe/figures/recycling_drivers_correlation.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe23496",
   "metadata": {},
   "source": [
    "Now, we move on to more specific recycling data that is broken down by material profile: wood, plastic, paper, packaging, metallic, glass. One of the most dramatic elements to be noticed immediately is that the recycling rate of plastic for Sweden went down dramatically to 28.6%. This is important in order to understand exactly which parts of the recycling industry have imporoved and where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the same 5 target countries for horizontal comparison\n",
    "targets = ['Germany', 'Sweden', 'Italy', 'Czechia', 'Greece']\n",
    "latest_year = res_d2_d3['year'].max()\n",
    "material_cols = [c for c in res_d2_d3.columns if 'recycling_rate_filled_' in c]\n",
    "material_labels = [m.replace('recycling_rate_filled_', '').title() for m in material_cols]\n",
    "\n",
    "fig, axes = plt.subplots(len(targets), 1, figsize=(12, len(targets) * 3), sharex=True)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "for i, country in enumerate(targets):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # extract data points\n",
    "    d2013 = res_d2_d3[\n",
    "        (res_d2_d3['country_name'] == country) & (res_d2_d3['year'] == 2013)\n",
    "    ][material_cols].values.flatten()\n",
    "\n",
    "    dLatest = res_d2_d3[\n",
    "        (res_d2_d3['country_name'] == country) & (res_d2_d3['year'] == latest_year)\n",
    "    ][material_cols].values.flatten()\n",
    "\n",
    "    y_pos = np.arange(len(material_labels))\n",
    "\n",
    "    # 2013 baseline\n",
    "    ax.barh(y_pos, d2013, color='#e0e0e0', label='2013 Rate', height=0.6)\n",
    "\n",
    "    # arrows for change\n",
    "    for j, (start, end) in enumerate(zip(d2013, dLatest)):\n",
    "        color = '#2ca02c' if end >= start else '#d62728'\n",
    "\n",
    "        ax.annotate(\n",
    "            '',\n",
    "            xy=(end, j),\n",
    "            xytext=(start, j),\n",
    "            arrowprops=dict(\n",
    "                arrowstyle='->',\n",
    "                color=color,\n",
    "                lw=3\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(material_labels, fontweight='bold', fontsize=11)\n",
    "    ax.set_title(\n",
    "        f\"Recycling Material Profile: {country}\",\n",
    "        fontsize=15,\n",
    "        fontweight='bold',\n",
    "        loc='left',\n",
    "        pad=10\n",
    "    )\n",
    "    ax.set_xlim(-5, 105)\n",
    "\n",
    "    # value labels\n",
    "    for j, (start, end) in enumerate(zip(d2013, dLatest)):\n",
    "        if end >= start:\n",
    "            # improvement â label on the right\n",
    "            x = end + 1\n",
    "            ha = 'left'\n",
    "        else:\n",
    "            # decline â label on the left\n",
    "            x = end - 1\n",
    "            ha = 'right'\n",
    "\n",
    "        ax.text(\n",
    "            x, j,\n",
    "            f\"{end:.1f}%\",\n",
    "            va='center',\n",
    "            ha=ha,\n",
    "            fontsize=10,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "\n",
    "\n",
    "    sns.despine(left=True, bottom=True)\n",
    "\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='#e0e0e0', lw=10, label='2013 Rate'),\n",
    "    Line2D([0], [0], color='#2ca02c', lw=3, label='Improvement'),\n",
    "    Line2D([0], [0], color='#d62728', lw=3, label='Decline')\n",
    "]\n",
    "\n",
    "fig.legend(\n",
    "    handles=legend_elements,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.02),\n",
    "    ncol=3,\n",
    "    frameon=False,\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../recycling-in-europe/figures/recycling_material_profile_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "\n",
    "### Data Aggregation & Selection\n",
    "\n",
    "Data averaged over **2019â2023** to mitigate annual volatility. Luxembourg is excluded due to statistical leverage (outlier GDP).\n",
    "\n",
    "**Variable Classification:**\n",
    "* **Targets:** Dependent variables (Recycling rates).\n",
    "* **Policy Drivers:** Modifiable independent variables (Taxes, Investment, Governance).\n",
    "* **Context:** Structural/Control variables (GDP, Education, Urbanization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "    'recycling_rate',\n",
    "    'recycling_rate_glass',\n",
    "    'recycling_rate_metallic',\n",
    "    'recycling_rate_packaging',\n",
    "    'recycling_rate_paper',\n",
    "    'recycling_rate_plastic',\n",
    "    'recycling_rate_wooden'\n",
    "]\n",
    "policy_drivers = [\n",
    "    'total_environm_tax_per_capita',\n",
    "    'pollut_environm_tax_per_capita',\n",
    "    'priv_inv_per_capita',\n",
    "    'government_effectiveness_estimate'\n",
    "]\n",
    "context_vars = [\n",
    "    'gdp_per_capita',\n",
    "    'gr_val_add_per_capita',\n",
    "    'urban_population_pct',\n",
    "    'renewable_energy_pct',\n",
    "    'highschool_completed_pct',\n",
    "    'household_exp_percapita'\n",
    "]\n",
    "\n",
    "start_year = 2019\n",
    "end_year = 2023\n",
    "\n",
    "df_all = pd.read_csv('processed_data/preprocessed_all.csv')\n",
    "df_recent = df_all[(df_all['year'] >= start_year) & (df_all['year'] <= end_year)].copy()\n",
    "\n",
    "all_vars = targets + policy_drivers + context_vars\n",
    "existing_cols = [c for c in all_vars if c in df_recent.columns]\n",
    "\n",
    "df_avg = df_recent.groupby('country_name')[existing_cols].mean().reset_index()\n",
    "df_avg = df_avg[df_avg['country_name'] != 'Luxembourg']\n",
    "regression_cols = ['recycling_rate'] + policy_drivers\n",
    "df_clean_regression = df_avg.dropna(subset=regression_cols).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Unsupervised Learning: Country Clustering\n",
    "\n",
    "**K-Means clustering** stratifies countries based on economic power (`gdp_per_capita`), outcome (`recycling_rate`), and institutional capacity (`government_effectiveness_estimate`).\n",
    "\n",
    "**Objective:**\n",
    "Identify structural heterogeneity. The scatter plot reveals performance tiers relative to wealth, isolating \"overachievers\" from resource-rich underperformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cluster = ['gdp_per_capita', 'recycling_rate', 'government_effectiveness_estimate']\n",
    "X_cluster = df_clean_regression[features_cluster].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "df_clean_regression['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=df_clean_regression,\n",
    "    x='gdp_per_capita',\n",
    "    y='recycling_rate',\n",
    "    hue='cluster',\n",
    "    palette='Set2',\n",
    "    s=200,\n",
    "    alpha=0.7,\n",
    "    edgecolor='black',\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "key_countries = ['Austria', 'Germany', 'Belgium', 'Spain', 'Italy', 'Poland', 'Romania', 'Bulgaria']\n",
    "for _, row in df_clean_regression.iterrows():\n",
    "    if row['country_name'] in key_countries:\n",
    "        plt.annotate(\n",
    "            row['country_name'],\n",
    "            xy=(row['gdp_per_capita'], row['recycling_rate']),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=9,\n",
    "            weight='bold'\n",
    "        )\n",
    "\n",
    "plt.title(\"Country Clusters: Wealth vs. Recycling Performance\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"GDP per Capita (â¬)\")\n",
    "plt.ylabel(\"General Recycling Rate (%)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(title='Cluster', loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figures/country_clusters.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCluster Profiles:\")\n",
    "print(df_clean_regression.groupby('cluster')[features_cluster].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "The clustering reveals that economic power is not the sole determinant of success. Cluster 1 combines high GDP (â¬53.5k) with strong governance (Index: 1.56) to maximize recycling (~52%). On the other hand, Cluster 0 and Cluster 2 share identical economic baselines (~â¬22k GDP) yet exhibit a 26.6% performance gap (41.3% vs. 14.7%).\n",
    "\n",
    "This disparity tracks with Government Effectiveness (0.82 vs. 0.44), indicating that for middle-income nations, policy enforcement is a more critical driver than raw capital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Multivariate Regression (OLS)\n",
    "\n",
    "Comparative OLS analysis quantifies driver impact. Two specifications are tested to resolve **multicollinearity**:\n",
    "\n",
    "1.  **Full Model (10 vars):** High variance inflation; prone to overfitting.\n",
    "2.  **Refined Model (5 vars):** Parsimonious selection representing distinct structural pillars:\n",
    "    * *Economic:* **Private Investment** (Capital flow).\n",
    "    * *Institutional:* **Government Effectiveness** (Enforcement capacity) and **Total Environment Tax** (Incentives by the government).\n",
    "    * *Social:* **Education** (Compliance proxy).\n",
    "\n",
    "**Visualization:** Standardized coefficients ($\\beta$) allow direct magnitude comparison across predictors with different units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'recycling_rate'\n",
    "predictors_full = policy_drivers + context_vars\n",
    "predictors_refined = [\n",
    "    'priv_inv_per_capita',\n",
    "    'total_environm_tax_per_capita',\n",
    "    'government_effectiveness_estimate',\n",
    "    'highschool_completed_pct'\n",
    "]\n",
    "\n",
    "X_full = df_clean_regression[predictors_full].copy()\n",
    "y = df_clean_regression[target].copy()\n",
    "\n",
    "X_full_std = (X_full - X_full.mean()) / X_full.std()\n",
    "X_full_std = sm.add_constant(X_full_std)\n",
    "\n",
    "model_full = sm.OLS(y, X_full_std).fit()\n",
    "\n",
    "X_ref = df_clean_regression[predictors_refined].copy()\n",
    "X_ref_std = (X_ref - X_ref.mean()) / X_ref.std()\n",
    "X_ref_std = sm.add_constant(X_ref_std)\n",
    "\n",
    "model_refined = sm.OLS(y, X_ref_std).fit()\n",
    "\n",
    "print(f\"Full model (10 vars)    -> R-squared Adj: {model_full.rsquared_adj:.3f} | AIC: {model_full.aic:.1f}\")\n",
    "print(f\"Refined model (4 vars) -> R-squared Adj: {model_refined.rsquared_adj:.3f} | AIC: {model_refined.aic:.1f}\")\n",
    "print(\"\\nWinning model: Refined model\")\n",
    "print(model_refined.summary())\n",
    "\n",
    "coefs = model_refined.params.drop('const').sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in coefs.values]\n",
    "\n",
    "coefs.plot(kind='barh', ax=ax, color=colors, edgecolor='black', width=0.7)\n",
    "\n",
    "ax.set_title(\"Key Drivers of Recycling Rate\\n(Refined Model - Standardized Coefficients)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Effect Size (Std. Deviations increase in Recycling Rate per 1 SD in predictor)\")\n",
    "ax.axvline(0, color='black', linewidth=1, linestyle='--')\n",
    "\n",
    "for index, value in enumerate(coefs):\n",
    "    offset = 0.1 if value > 0 else -0.5\n",
    "    ax.text(value + offset, index, f\"{value:.2f}\", va='center', fontweight='bold', color='#333333')\n",
    "\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"./figures/refined_model_drivers.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "The refined model (4 vars) reduces noise, improving Adjusted  (0.40 vs. 0.37) compared to the full specification. \n",
    "\n",
    "The key driver for recycling is **Private Investment** ($\\beta=7.49$) and is the only statistically significant predictor ($p=0.05$), identifying capital allocation as the primary lever for recycling performance. Education contributes marginally ($\\beta=4.56$); conversely, environmental taxes and government effectiveness show no independent statistical significance in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### Bivariate Analysis: Fiscal Policy\n",
    "\n",
    "Pearson correlation ($r$) assesses the relationship between taxation subtypes and recycling performance.\n",
    "\n",
    "**Distinction:**\n",
    "* **Total Environmental Tax:** Broad fiscal policy/infrastructure funding.\n",
    "* **Pollution Tax:** Specific levies on emissions. often **reactive** measures in high-pollution contexts rather than preventative drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'recycling_rate'\n",
    "tax_vars = ['total_environm_tax_per_capita', 'pollut_environm_tax_per_capita']\n",
    "df_tax = df_all[['country_name', 'year', target] + tax_vars].copy()\n",
    "\n",
    "df_tax[tax_vars + [target]] = df_tax.groupby('country_name')[tax_vars + [target]].transform(\n",
    "    lambda x: x.interpolate(limit_direction='both')\n",
    ")\n",
    "df_tax = df_tax.dropna()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.regplot(x='total_environm_tax_per_capita', y='recycling_rate', data=df_tax, \n",
    "            ax=ax1, scatter_kws={'alpha':0.4, 'color':'tab:blue'}, line_kws={'color':'red'})\n",
    "\n",
    "r_tot, p_tot = pearsonr(df_tax['total_environm_tax_per_capita'], df_tax['recycling_rate'])\n",
    "ax1.set_title(f'Total Environmental Tax Impact\\nR = {r_tot:.2f} (p={p_tot:.3f})', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Total Environmental Tax (â¬ per capita)')\n",
    "ax1.set_ylabel('Recycling Rate (%)')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "sns.regplot(x='pollut_environm_tax_per_capita', y='recycling_rate', data=df_tax, \n",
    "            ax=ax2, scatter_kws={'alpha':0.4, 'color':'tab:green'}, line_kws={'color':'darkgreen'})\n",
    "\n",
    "r_pol, p_pol = pearsonr(df_tax['pollut_environm_tax_per_capita'], df_tax['recycling_rate'])\n",
    "ax2.set_title(f'Pollution Tax Impact\\nR = {r_pol:.2f} (p={p_pol:.3f})', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Pollution Environmental Tax (â¬ per capita)')\n",
    "ax2.set_ylabel('')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figures/tax_correlation_comparison.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "Data reveals that direct pollution taxes comprise a minor share of fiscal policy. High performers like Germany maintain low direct levies despite superior recycling outcomes.\n",
    "The weak positive association $(R=0.26)$ contrasts with broader environmental taxation $(R=0.68)$, indicating that pollution-specific levies are currently under-leveraged compared to general infrastructure funding.\n",
    "\n",
    "As a policy recommendation, there is significant scope to implement targeted pollution taxes. Shifting fiscal pressure from general revenue to specific behavioral penalties represents an unexploited mechanism to further accelerate circular transition. The green line shows the correlation for those countries that do implement said taxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "# Dataset preparation for recycling rate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_drop = [\n",
    "    \"Bosnia and Herzegovina\",\n",
    "    \"Montenegro\",\n",
    "    \"North Macedonia\",\n",
    "    \"TÃ¼rkiye\",\n",
    "    \"Serbia\",\n",
    "    \"Albania\"\n",
    "]\n",
    "\n",
    "res_d1_d3 = res_d1_d3[\n",
    "    ~res_d1_d3[\"country_name\"].isin(countries_to_drop)\n",
    "].copy()\n",
    "\n",
    "display(res_d1_d3)\n",
    "sorted(res_d1_d3[\"country_name\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_horizon_delta_dataset(\n",
    "    df,\n",
    "    horizon=7,\n",
    "    start_year=2000,\n",
    "    end_year=None,\n",
    "    target_col=\"recycling_rate_filled\",\n",
    "    feature_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a dataset where the target is the change in target_col over `horizon` years.\n",
    "    \"\"\"\n",
    "\n",
    "    if end_year is None:\n",
    "        end_year = df[\"year\"].max() - horizon\n",
    "\n",
    "    # anchor data (t)\n",
    "    df_t = df[\n",
    "        (df[\"year\"] >= start_year) &\n",
    "        (df[\"year\"] <= end_year)\n",
    "    ][[\"country_name\", \"year\"] + feature_cols].copy()\n",
    "\n",
    "    # future data (t + horizon)\n",
    "    df_t_future = df[\n",
    "        (df[\"year\"] >= start_year + horizon) &\n",
    "        (df[\"year\"] <= end_year + horizon)\n",
    "    ][[\"country_name\", \"year\", target_col]].copy()\n",
    "\n",
    "    df_t_future[\"year\"] -= horizon\n",
    "\n",
    "    # merge\n",
    "    merged = df_t.merge(\n",
    "        df_t_future,\n",
    "        on=[\"country_name\", \"year\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_future\")\n",
    "    )\n",
    "\n",
    "    # delta target\n",
    "    merged[f\"delta_{target_col}_{horizon}yr\"] = (\n",
    "        merged[f\"{target_col}_future\"] - merged[target_col]\n",
    "    )\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"recycling_rate_filled\",\n",
    "    \"gdp_per_capita_filled\",\n",
    "    \"urban_population_pct_filled\",\n",
    "    \"internet_users_pct_filled\",\n",
    "    \"renewable_energy_pct_filled\",\n",
    "    \"tourism_arrivals_filled\",\n",
    "    \"population_total_filled\",\n",
    "    \"manufacturing_value_added_pct_gdp_filled\",\n",
    "    \"government_effectiveness_estimate_filled\",\n",
    "    \"highschool_completed_pct_filled\",\n",
    "    \"household_exp_percapita_filled\"\n",
    "]\n",
    "\n",
    "# build dataset with horizon recycling rates (in 7 years)\n",
    "df_7yr = build_horizon_delta_dataset(\n",
    "    feature_cols = feature_cols,\n",
    "    df=res_d1_d3,\n",
    "    horizon=7\n",
    ")\n",
    "\n",
    "display(df_7yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df_7yr[\"delta_recycling_rate_filled_7yr\"], bins=30)\n",
    "plt.xlabel(\"7-year change in recycling rate\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of 7-year recycling rate changes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "## Check for diminsihing returns \n",
    "\n",
    "We want to check that higher starting rate â smaller improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df_7yr[\"recycling_rate_filled\"],\n",
    "    df_7yr[\"delta_recycling_rate_filled_7yr\"],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.xlabel(\"Initial recycling rate\")\n",
    "plt.ylabel(\"7-year change\")\n",
    "plt.title(\"Diminishing returns check\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "## Split the data for delta = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation/test datasets\n",
    "train_df = df_7yr[df_7yr[\"year\"] <= 2012].copy()\n",
    "val_df   = df_7yr[df_7yr[\"year\"] > 2012].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining feature and target variables\n",
    "target = \"delta_recycling_rate_filled_7yr\"\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target]\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "## Baseline Model: Linear regression (with regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Build a regression pipeline that scales features and fits a Ridge regression model\n",
    "baseline_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Generate predictions on validation set and evaluate model performance using MAE and RÂ²\n",
    "y_pred = baseline_model.predict(X_val)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred))\n",
    "print(\"RÂ²:\", r2_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coefficients of the regression model\n",
    "coefs = pd.Series(\n",
    "    baseline_model.named_steps[\"model\"].coef_,\n",
    "    index=X_train.columns\n",
    ").sort_values()\n",
    "\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted over actual 7-year change\n",
    "plt.scatter(y_val, y_pred, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-year change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "\n",
    "# Define helper functions\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, name=\"model\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    r2 = r2_score(y_val, preds)\n",
    "    return mae, r2, model, preds\n",
    "\n",
    "def cast_int_or_none(x):\n",
    "    return None if pd.isna(x) else int(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "## Non-linear Model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basline RF model \n",
    "rf_model = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "\n",
    "rf_mae, rf_r2, _, _ = evaluate_model(rf_model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"RF Baseline MAE:\", rf_mae)\n",
    "print(\"RF Baseline RÂ²:\", rf_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual tuning to optimize hyperparameters (n_estimators, max_depth, min_samples_leaf, min_samples_split, max_features)\n",
    "results = []\n",
    "\n",
    "for n_estimators in [200, 400, 800]:\n",
    "    for max_depth in [None, 8, 12]:\n",
    "        for min_leaf in [1, 2, 4]:\n",
    "            for min_split in [2, 5, 10]:\n",
    "                for max_feat in [\"sqrt\", \"log2\", None]:\n",
    "\n",
    "                    rf = RandomForestRegressor(\n",
    "                        n_estimators=n_estimators,\n",
    "                        max_depth=max_depth,\n",
    "                        min_samples_leaf=min_leaf,\n",
    "                        min_samples_split=min_split,\n",
    "                        max_features=max_feat,\n",
    "                        random_state=42,\n",
    "                    )\n",
    "\n",
    "                    mae, r2, _, _ = evaluate_model(rf, X_train, y_train, X_val, y_val)\n",
    "\n",
    "                    results.append({\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"min_samples_leaf\": min_leaf,\n",
    "                        \"min_samples_split\": min_split,\n",
    "                        \"max_features\": max_feat,\n",
    "                        \"MAE\": mae,\n",
    "                        \"R2\": r2\n",
    "                    })\n",
    "\n",
    "rf_results = pd.DataFrame(results).sort_values(\"MAE\")\n",
    "\n",
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate best performing RF model (MAE, R2 & Importances)\n",
    "best_rf = RandomForestRegressor(\n",
    "    n_estimators=rf_results.iloc[0][\"n_estimators\"],\n",
    "    max_depth=cast_int_or_none(rf_results.iloc[0][\"max_depth\"]),\n",
    "    min_samples_leaf=rf_results.iloc[0][\"min_samples_leaf\"],\n",
    "    min_samples_split=rf_results.iloc[0][\"min_samples_split\"],\n",
    "    max_features=rf_results.iloc[0][\"max_features\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_rf_mae, best_rf_r2, best_rf, best_rf_preds = evaluate_model(best_rf, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Best RF MAE:\", best_rf_mae)\n",
    "print(\"Best RF RÂ²:\", best_rf_r2)\n",
    "\n",
    "plt.scatter(y_val, best_rf_preds, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-year change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(best_rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "## Non-linear Model: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline Gradient Boosting model\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbr_mae, gbr_r2, _, _ = evaluate_model(gbr, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"GBR Baseline MAE:\", gbr_mae)\n",
    "print(\"GBR Baseline RÂ²:\", gbr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual tuning to optimize hyperparameters (n_estimators, learning_rate, max_depth, min_samples_leaf, subsample)\n",
    "\n",
    "gbr_results = []\n",
    "\n",
    "for n_estimators in [200, 400, 800]:\n",
    "    for learning_rate in [0.03, 0.05, 0.1]:\n",
    "        for max_depth in [2, 3, 4]:\n",
    "            for min_leaf in [1, 3, 5]:\n",
    "                for subsample in [0.7, 0.9, 1.0]:\n",
    "\n",
    "                    gbr = GradientBoostingRegressor(\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth,\n",
    "                        min_samples_leaf=min_leaf,\n",
    "                        subsample=subsample,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    \n",
    "                    mae, r2, _, _ = evaluate_model(gbr, X_train, y_train, X_val, y_val)\n",
    "\n",
    "                    gbr_results.append({\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"min_samples_leaf\": min_leaf,\n",
    "                        \"subsample\": subsample,\n",
    "                        \"MAE\": mae,\n",
    "                        \"R2\": r2\n",
    "                    })\n",
    "\n",
    "gbr_results_df = (\n",
    "    pd.DataFrame(gbr_results)\n",
    "    .sort_values(\"MAE\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "gbr_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate best performing GBR model (MAE, R2 & Importances)\n",
    "\n",
    "best_gbr = GradientBoostingRegressor(\n",
    "    n_estimators=cast_int_or_none(gbr_results_df.iloc[0][\"n_estimators\"]),\n",
    "    learning_rate=gbr_results_df.iloc[0][\"learning_rate\"],\n",
    "    max_depth=cast_int_or_none(gbr_results_df.iloc[0][\"max_depth\"]),\n",
    "    min_samples_leaf=cast_int_or_none(gbr_results_df.iloc[0][\"min_samples_leaf\"]),\n",
    "    subsample=gbr_results_df.iloc[0][\"subsample\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_gbr_mae, best_gbr_r2, best_gbr, best_gbr_preds = evaluate_model(best_gbr, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Best GBR MAE:\", best_gbr_mae)\n",
    "print(\"Best GBR RÂ²:\", best_gbr_r2)\n",
    "\n",
    "plt.scatter(y_val, best_gbr_preds, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-year change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(best_gbr.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "## Non-linear Model: Histogram-based Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basemodel HGB model\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    loss=\"absolute_error\",\n",
    "    max_iter=300,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hgb_mae, hgb_r2, _, _ = evaluate_model(hgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "print(\"HGB Baseline MAE:\", hgb_mae)\n",
    "print(\"HGB Baseline RÂ²:\", hgb_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual tuning to optimize hyperparameters (max_iter, learning_rate, max_depth, min_samples_leaf)\n",
    "\n",
    "hgb_results = []\n",
    "\n",
    "for max_iter in [300, 600, 1000]:\n",
    "    for learning_rate in [0.03, 0.05, 0.1]:\n",
    "        for max_depth in [3, 5, 7]:\n",
    "            for min_samples_leaf in [10, 20, 40]:\n",
    "\n",
    "                hgb = HistGradientBoostingRegressor(\n",
    "                    max_iter=max_iter,\n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    random_state=42\n",
    "                )\n",
    "\n",
    "                mae, r2, _, _ = evaluate_model(hgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "                hgb_results.append({\n",
    "                    \"max_iter\": max_iter,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"max_depth\": max_depth,\n",
    "                    \"min_samples_leaf\": min_samples_leaf,\n",
    "                    \"MAE\": mae,\n",
    "                    \"R2\": r2\n",
    "                })\n",
    "\n",
    "hgb_results_df = (\n",
    "    pd.DataFrame(hgb_results)\n",
    "    .sort_values(\"MAE\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "hgb_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate best performing HGB model (MAE, R2)\n",
    "\n",
    "best_hgb = HistGradientBoostingRegressor(\n",
    "    max_iter=cast_int_or_none(hgb_results_df.iloc[0][\"max_iter\"]),\n",
    "    learning_rate=hgb_results_df.iloc[0][\"learning_rate\"],\n",
    "    max_depth=cast_int_or_none(hgb_results_df.iloc[0][\"max_depth\"]),\n",
    "    min_samples_leaf=cast_int_or_none(hgb_results_df.iloc[0][\"min_samples_leaf\"]),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_hgb_mae, best_hgb_r2, best_hgb, best_hgb_preds = evaluate_model(best_hgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Best HGB MAE:\", best_hgb_mae)\n",
    "print(\"Best hgb RÂ²:\", best_hgb_r2)\n",
    "\n",
    "plt.scatter(y_val, best_hgb_preds, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-year change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "## Non-linear Model: Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define Baseline XGB model\n",
    "xgb = XGBRegressor(\n",
    "    objective=\"reg:absoluteerror\", \n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_mae, xgb_r2, _, _ = evaluate_model(xgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"XGB Baseline MAE:\", xgb_mae)\n",
    "print(\"XGB Baseline RÂ²:\", xgb_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Manual tuning to optimize hyperparameters (n_estimators, learning_rate, max_depth, subsample, colsample_bytree)\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "for n_estimators in [300, 600, 1000]:\n",
    "    for learning_rate in [0.03, 0.05, 0.1]:\n",
    "        for max_depth in [3, 4, 5]:\n",
    "            for subsample in [0.7, 0.9, 1.0]:\n",
    "                for colsample in [0.7, 0.9, 1.0]:\n",
    "\n",
    "                    xgb = XGBRegressor(\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth,\n",
    "                        subsample=subsample,\n",
    "                        colsample_bytree=colsample,\n",
    "                        random_state=42,\n",
    "                    )\n",
    "\n",
    "                    mae, r2, _, _ = evaluate_model(xgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "                    xgb_results.append({\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"subsample\": subsample,\n",
    "                        \"colsample_bytree\": colsample,\n",
    "                        \"MAE\": mae,\n",
    "                        \"R2\": r2\n",
    "                    })\n",
    "\n",
    "xgb_results_df = (\n",
    "    pd.DataFrame(xgb_results)\n",
    "    .sort_values(\"MAE\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "xgb_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate best performing XGB model (MAE, R2 & Importances)\n",
    "\n",
    "best_xgb = XGBRegressor(\n",
    "    n_estimators=cast_int_or_none(xgb_results_df.iloc[0][\"n_estimators\"]),\n",
    "    learning_rate=xgb_results_df.iloc[0][\"learning_rate\"],\n",
    "    max_depth=cast_int_or_none(xgb_results_df.iloc[0][\"max_depth\"]),\n",
    "    subsample=xgb_results_df.iloc[0][\"subsample\"],\n",
    "    colsample_bytree=xgb_results_df.iloc[0][\"colsample_bytree\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_xgb_mae, best_xgb_r2, best_xgb, best_xgb_preds = evaluate_model(best_xgb, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Best XGB MAE:\", best_xgb_mae)\n",
    "print(\"Best XGB RÂ²:\", best_xgb_r2)\n",
    "\n",
    "plt.scatter(y_val, best_xgb_preds, alpha=0.6)\n",
    "plt.plot([y_val.min(), y_val.max()],\n",
    "         [y_val.min(), y_val.max()],\n",
    "         \"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual 7-yeaxr change\")\n",
    "plt.ylabel(\"Predicted 7-year change\")\n",
    "plt.title(\"Validation: Predicted vs Actual\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(best_xgb.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "## Would we see better MAE with 5 years delta (instead of 7)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resplitting dataset for delta of 5 years\n",
    "\n",
    "df_5yr = build_horizon_delta_dataset(\n",
    "    feature_cols=feature_cols,\n",
    "    df=res_d1_d3,\n",
    "    horizon=5\n",
    ")\n",
    "\n",
    "train_df_5 = df_5yr[df_5yr[\"year\"] <= 2014]\n",
    "val_df_5   = df_5yr[df_5yr[\"year\"] > 2014]\n",
    "\n",
    "target_5 = \"delta_recycling_rate_filled_5yr\"\n",
    "\n",
    "X_train_5 = train_df_5[feature_cols]\n",
    "y_train_5 = train_df_5[target_5]\n",
    "\n",
    "X_val_5 = val_df_5[feature_cols]\n",
    "y_val_5 = val_df_5[target_5]\n",
    "\n",
    "# Evaluatign previous optimized models for delta of 5 years\n",
    "best_rf_5_mae , best_rf_5_r2, _, _ = evaluate_model(best_rf,  X_train_5, y_train_5, X_val_5, y_val_5, \"RF (5yr)\")\n",
    "best_gbr_5_mae , best_gbr_5_r2, _, _ =evaluate_model(best_gbr, X_train_5, y_train_5, X_val_5, y_val_5, \"GBR (5yr)\")\n",
    "best_hgb_5_mae , best_hgb_5_r2, _, _ =evaluate_model(best_hgb, X_train_5, y_train_5, X_val_5, y_val_5, \"HGB (5yr)\")\n",
    "best_xgb_5_mae , best_xgb_5_r2, _, _ =evaluate_model(best_xgb, X_train_5, y_train_5, X_val_5, y_val_5, \"XGB (5yr)\")\n",
    "\n",
    "results_5yr = {\n",
    "    \"RF (5yr)\": (best_rf_5_mae, best_rf_5_r2),\n",
    "    \"GBR (5yr)\": (best_gbr_5_mae, best_gbr_5_r2),\n",
    "    \"HGB (5yr)\": (best_hgb_5_mae, best_hgb_5_r2),\n",
    "    \"XGB (5yr)\": (best_xgb_5_mae, best_xgb_5_r2),\n",
    "}\n",
    "\n",
    "for model, (mae, r2) in results_5yr.items():\n",
    "    print(f\"{model:10s} | MAE: {mae:.3f} | RÂ²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "Answer is no but we might need to retune the models for 5 years delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "# Predict recycling rates for dataset in 2030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions__df = res_d1_d3[res_d1_d3[\"year\"] == res_d1_d3[\"year\"].max()].copy()\n",
    "\n",
    "# Features for prediction\n",
    "X_latest = predictions__df[feature_cols]\n",
    "\n",
    "# Predict 7-year delta from latest year\n",
    "predicted_delta = best_xgb.predict(X_latest)\n",
    "\n",
    "# Compute projected 2030 rate\n",
    "predictions__df[\"predicted_delta_2030\"] = predicted_delta\n",
    "predictions__df[\"predicted_rate_2030\"] = predictions__df[\"recycling_rate_filled\"] + predicted_delta\n",
    "\n",
    "eu_target = 60 \n",
    "predictions__df[\"meets_target\"] = predictions__df[\"predicted_rate_2030\"] >= eu_target\n",
    "\n",
    "projection_2030 = predictions__df[\n",
    "    [\"country_name\", \"recycling_rate_filled\", \"predicted_delta_2030\", \"predicted_rate_2030\", \"meets_target\"]\n",
    "].sort_values(\"predicted_rate_2030\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "projection_2030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show predicted deltas by 2030\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(projection_2030[\"predicted_delta_2030\"], bins=7, kde=True, color='skyblue')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted 7-year increase in recycling rate\")\n",
    "plt.ylabel(\"Number of countries\")\n",
    "plt.title(\"Distribution of Predicted Recycling Rate Increase by 2030\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predicted final recycling rates by 2030\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(projection_2030[\"predicted_rate_2030\"], bins=9, kde=True, color='green')\n",
    "plt.axvline(eu_target, color='red', linestyle='--', label=f'EU Target ({eu_target}%)')\n",
    "plt.xlabel(\"Predicted recycling rate in 2030 (%)\")\n",
    "plt.ylabel(\"Number of countries\")\n",
    "plt.title(\"Distribution of Predicted Recycling Rates in 2030\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Current vs preidcted recycling rates by 2030\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(\n",
    "    x=\"recycling_rate_filled\",\n",
    "    y=\"predicted_rate_2030\",\n",
    "    hue=\"meets_target\",\n",
    "    data=projection_2030,\n",
    "    palette={True: 'green', False: 'red'},\n",
    "    s=100\n",
    ")\n",
    "plt.plot([0, 100], [0, 100], 'k--', alpha=0.7)  # diagonal\n",
    "plt.xlabel(\"Current recycling rate (%)\")\n",
    "plt.ylabel(\"Predicted rate in 2030 (%)\")\n",
    "plt.title(\"Current vs Predicted Recycling Rates\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "## Countries with biggest predicted improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show countries with biggets predicted improvements \n",
    "top_increases = projection_2030.sort_values(\"predicted_delta_2030\", ascending=False).head(10)\n",
    "top_increases[[\"country_name\", \"recycling_rate_filled\", \"predicted_delta_2030\", \"predicted_rate_2030\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "## Countries by outcome analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most contriubting variables to recycling rates deltas\n",
    "importances = best_xgb.feature_importances_\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feat_imp_df, palette=\"viridis\")\n",
    "plt.title(\"Global Feature Importance (XGBoost)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(best_xgb, X_latest)\n",
    "shap_values = explainer(X_latest)\n",
    "\n",
    "# Global summary\n",
    "shap.summary_plot(shap_values, X_latest, feature_names=feature_cols)\n",
    "\n",
    "# Mean absolute SHAP value per feature\n",
    "shap_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"mean_abs_shap\": np.abs(shap_values.values).mean(axis=0)\n",
    "}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "print(shap_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Split dataset into achievers and non-achievers\n",
    "achievers = predictions__df[predictions__df[\"meets_target\"]].copy()\n",
    "non_achievers = predictions__df[~predictions__df[\"meets_target\"]].copy()\n",
    "\n",
    "# Determine grid size\n",
    "n_features = len(feature_cols)\n",
    "n_cols = 3 \n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
    "axes = axes.flatten()  # flatten for easy indexing\n",
    "\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    sns.kdeplot(achievers[feature], label=\"Achievers\", fill=True, ax=axes[i])\n",
    "    sns.kdeplot(non_achievers[feature], label=\"Non-Achievers\", fill=True, ax=axes[i])\n",
    "    axes[i].set_title(f\"{feature} distribution\")\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Remove empty subplots if any\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exdex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
